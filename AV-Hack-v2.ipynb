{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next Steps**\n",
    "- Remove softmax\n",
    "- Combine 3 models by directly summing their predictions with linal norm\n",
    "\n",
    "- choose between RF and XGB\n",
    "- choose between M1 variants\n",
    "- Hierarchical model With custom proba for class 2\n",
    "- Improve Sampling Strategy\n",
    "\n",
    "\n",
    "- Add SGD+LR Scheduler\n",
    "- **Fix OLR**\n",
    "- Use trained embedding from one simple model to another?\n",
    "\n",
    "- optimal thresholding\n",
    "\n",
    "\n",
    "**Aces**\n",
    "\n",
    "- dropout and regularization\n",
    "- OLR+SGD+momentum or LR Scheduling\n",
    "- Architecture and Multiple Receptive Field\n",
    "    - Go Wide\n",
    "    - Wider Embedding as well\n",
    "- HyperParams from Params Section\n",
    "- Try SGD with LR scheduling\n",
    "- Try with and without removing hypen `-`\n",
    "- Optimal thresholding for F1\n",
    "- epochs based on val acc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features**\n",
    "- Average Rating of Drug\n",
    "- don't do stop words\n",
    "- Use Drug Column through embedding layer as well to get Drug Vector.\n",
    "- Run Fasttext/Glove on this corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    " \n",
    "# The GPU id to use, usually either \"0\" or \"1\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\";  \n",
    " \n",
    "# Do other imports now...\n",
    "import keras\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available()\n",
    "\n",
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.backend.tensorflow_backend import clear_session\n",
    "from keras.backend.tensorflow_backend import get_session\n",
    "import tensorflow\n",
    "\n",
    "# Reset Keras Session\n",
    "def reset_keras():\n",
    "    sess = get_session()\n",
    "    clear_session()\n",
    "    sess.close()\n",
    "    sess = get_session()\n",
    "\n",
    "    try:\n",
    "        del classifier # this is from global space - change this as you need\n",
    "        del model\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    print(gc.collect()) # if it's done something you should see a number being outputted\n",
    "\n",
    "    # use the same config as you used to create the session\n",
    "    config = tensorflow.ConfigProto(log_device_placement=True, allow_soft_placement=True,)\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "    config.gpu_options.visible_device_list = \"0\"\n",
    "    set_session(tensorflow.Session(config=config))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n"
     ]
    }
   ],
   "source": [
    "reset_keras()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T16:20:07.179065Z",
     "start_time": "2019-07-24T16:20:01.945144Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'data_science_utils.models' from '/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/data_science_utils/models/__init__.py'>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<module 'lib' from '/home/ec2-user/SageMaker/lib.py'>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<module 'params' from '/home/ec2-user/SageMaker/params.py'>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<module 'data_gen' from '/home/ec2-user/SageMaker/data_gen.py'>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Reshape, Multiply\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "%matplotlib inline\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, DepthwiseConv2D, Conv2D, SeparableConv2D, MaxPooling1D, AveragePooling1D\n",
    "from keras.layers import Input, concatenate, LeakyReLU\n",
    "import gensim.downloader as api\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Activation, Flatten, Dense, Dropout, SpatialDropout1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD, Nadam, Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "from keras.regularizers import l2\n",
    "%config InlineBackend.figure_format='retina'\n",
    "from keras_contrib.callbacks import CyclicLR\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from data_science_utils.vision.keras import *\n",
    "from time import time\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import missingno as msno\n",
    "import re\n",
    "from joblib import Parallel, delayed\n",
    "from data_science_utils import dataframe as df_utils\n",
    "from data_science_utils import models as model_utils\n",
    "from data_science_utils import plots as plot_utils\n",
    "from data_science_utils.dataframe import column as column_utils\n",
    "from data_science_utils import misc as misc\n",
    "from data_science_utils import preprocessing as pp_utils\n",
    "from data_science_utils import nlp as nlp_utils\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from data_science_utils.dataframe import get_specific_cols\n",
    "from random import sample\n",
    "\n",
    "import more_itertools\n",
    "from more_itertools import flatten\n",
    "import ast\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import gc\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd())\n",
    "from importlib import reload\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "reload(model_utils)\n",
    "from data_science_utils.models import mean_absolute_percentage_error\n",
    "from data_science_utils.models import median_absolute_percentage_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "from more_itertools import flatten\n",
    "import dill\n",
    "from collections import Counter\n",
    "import operator\n",
    "from gensim.models import FastText\n",
    "import itertools\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.utils import tokenize\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "\n",
    "import lib\n",
    "reload(lib)\n",
    "from lib import *\n",
    "\n",
    "import params\n",
    "reload(params)\n",
    "from params import *\n",
    "\n",
    "import data_gen\n",
    "reload(data_gen)\n",
    "from data_gen import *\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from keras.regularizers import l2\n",
    "from keras.regularizers import L1L2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = api.load(\"glove-twitter-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fname = os.getcwd()+\"/fasttext.model\"\n",
    "from gensim.models import FastText\n",
    "fb_model = FastText.load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T17:35:47.727614Z",
     "start_time": "2019-07-24T17:35:47.363819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5279, 16)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2924, 14)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = read_csv(\"train_padded.csv\")\n",
    "df_test = read_csv(\"test_padded.csv\")\n",
    "\n",
    "\n",
    "df.shape\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T17:39:49.289992Z",
     "start_time": "2019-07-24T17:39:48.120167Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25548"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "18769"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Exclusive Distinct words =  4267\n",
      "Total vocab size =  29815\n"
     ]
    }
   ],
   "source": [
    "# How many words in test set are not in train set\n",
    "\n",
    "train_words = set(list(more_itertools.flatten(df.full_txt.values)))\n",
    "test_words = set(list(more_itertools.flatten(df_test.full_txt.values)))\n",
    "all_words = set(list(more_itertools.flatten(df.full_txt.values))+list(more_itertools.flatten(df_test.full_txt.values)))\n",
    "len(train_words)\n",
    "len(test_words)\n",
    "test_exlusive_words = test_words-train_words\n",
    "\n",
    "print(\"Test Set Exclusive Distinct words = \",len(test_exlusive_words))\n",
    "\n",
    "print(\"Total vocab size = \",len(all_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcess for M1 and M5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T17:39:53.844047Z",
     "start_time": "2019-07-24T17:39:51.771975Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8203/8203 [00:00<00:00, 22926.00it/s]\n",
      " 16%|█▋        | 869/5279 [00:00<00:00, 8687.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words before Min frequency filtering 29815\n",
      "Total Words after Min frequency filtering 11859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5279/5279 [00:00<00:00, 8324.71it/s]\n",
      "100%|██████████| 2924/2924 [00:00<00:00, 8388.86it/s]\n",
      "100%|██████████| 5279/5279 [00:00<00:00, 27034.78it/s]\n",
      "100%|██████████| 2924/2924 [00:00<00:00, 31016.20it/s]\n"
     ]
    }
   ],
   "source": [
    "le_train,le_transform, le = get_text_le(vocab_size=vocab_size, min_count=min_count)\n",
    "_ = le_train(list(df.full_txt.values)+list(df_test.full_txt.values))\n",
    "\n",
    "df[\"full_txt_encoded\"] = le_transform(df.full_txt.values)\n",
    "df_test[\"full_txt_encoded\"] = le_transform(df_test.full_txt.values)\n",
    "\n",
    "df[\"context_txt_encoded\"] = le_transform(df.context_txt.values)\n",
    "df_test[\"context_txt_encoded\"] = le_transform(df_test.context_txt.values)\n",
    "\n",
    "def do_padding(df):\n",
    "    df['full_txt_encoded'] = list(pad_text_sequences(df['full_txt_encoded'].values,empty=0, maxlen=full_txt_maxlen,jobs=jobs))\n",
    "    df['context_txt_encoded'] = list(pad_text_sequences(df['context_txt_encoded'].values,empty=0, maxlen=context_txt_maxlen,jobs=jobs))\n",
    "    return df\n",
    "\n",
    "df = do_padding(df)\n",
    "df_test = do_padding(df_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T17:39:54.125578Z",
     "start_time": "2019-07-24T17:39:54.094245Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.rand(vocab_size, embedding_dims)*0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T17:39:54.935701Z",
     "start_time": "2019-07-24T17:39:54.128958Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient 8\n",
      "[-1.3836      0.71107    -1.83200002  0.65379     0.74203002 -0.1154\n",
      "  0.47209999 -0.58218998  0.034721    0.17285    -0.51932001  0.26039001\n",
      " -2.91829991  0.67479998  0.38541001  0.23262     0.39111     0.49118999\n",
      " -0.043787   -1.05869997  0.71371001 -0.11127    -1.02769995  0.13327\n",
      " -0.60834998]\n"
     ]
    }
   ],
   "source": [
    "not_printed=True\n",
    "for w in le[\"wd\"].keys():\n",
    "    try:\n",
    "        embedding_matrix[le[\"wd\"][w]] = glove.wv[w][:embedding_dims]\n",
    "        if not_printed:\n",
    "            not_printed=False\n",
    "            print(w,le[\"wd\"][w])\n",
    "            print(embedding_matrix[le[\"wd\"][w]])\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_caliberation_wrapper(arr,weights=[1,1,1]):\n",
    "    arr = np.copy(arr)\n",
    "    for i,v in enumerate(weights):\n",
    "        arr[:,i] = arr[:,i]*v\n",
    "    return arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_block(x,layer_width,dropout=0):\n",
    "    x1 = conv_layer(x,n_kernels=16*layer_width,kernel_size=5,padding='same')\n",
    "    x2 = conv_layer(x1,n_kernels=16*layer_width,kernel_size=3,padding='same')\n",
    "    x = concatenate([x1,x2])\n",
    "    x3 = conv_layer(x,n_kernels=8*layer_width,kernel_size=3,padding='same')\n",
    "    \n",
    "    x = concatenate([x1,x2,x3])\n",
    "    x = transition_layer(x, n_kernels=16*layer_width, dropout=0)\n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T17:41:15.134250Z",
     "start_time": "2019-07-24T17:41:15.114251Z"
    }
   },
   "outputs": [],
   "source": [
    "def conv_pipe(x,layer_width,dropout=0):\n",
    "    \n",
    "    x1 = dense_block(x,layer_width)\n",
    "    xg0 = GlobalAveragePooling1D()(x1)\n",
    "    x2 = dense_block(x1,layer_width)\n",
    "    xg1 = GlobalAveragePooling1D()(x2)\n",
    "    \n",
    "    # x3 = MaxPooling1D()(x2)\n",
    "    x4 = dense_block(x2,layer_width)\n",
    "    xg2 = GlobalAveragePooling1D()(x4)\n",
    "    return concatenate([xg0,xg1, xg2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T17:41:15.205637Z",
     "start_time": "2019-07-24T17:41:15.171080Z"
    }
   },
   "outputs": [],
   "source": [
    "def resample_input_for_training(x):\n",
    "    xs1 = x[x['sentiment']==0]\n",
    "    xs2 = x[x['sentiment']==1]\n",
    "    x = pd.concat([x,xs1.sample(frac=1),xs1.sample(frac=1),xs1.sample(frac=1),xs1.sample(frac=1),xs1.sample(frac=1),xs1.sample(frac=1)])\n",
    "    x = pd.concat([x,xs2.sample(frac=1),xs2.sample(frac=1),xs2.sample(frac=1),xs2.sample(frac=1)])\n",
    "    x = x.sample(frac=1)\n",
    "    \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator_model_with_validation(model,train,val=None,extract_values=None,\n",
    "                                          epochs=None, batch_size=None,\n",
    "                                          policy=None,lr=0.01,max_lr_olr=0.005, \n",
    "                                          lr_shed_fn=lambda e,lr:0.95*lr):\n",
    "    # policy olr+sgd | sgd+lrs | adam\n",
    "    train_copy = train.copy()\n",
    "    train = resample_input_for_training(train)\n",
    "    train_iterator = MakeIter(train,extract_values,batch_size)\n",
    "    print(\"Before Resampling, Size = \",train_copy.shape,\"\\n\",train_copy.sentiment.value_counts())\n",
    "    print(\"After Resampling = \",train.shape,\"\\n\",train.sentiment.value_counts())\n",
    "    \n",
    "    \n",
    "    # checkpoint = ModelCheckpoint(\"model.hdf5\", monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n",
    "    \n",
    "    assert epochs is not None\n",
    "    assert policy is not None\n",
    "    assert batch_size is not None\n",
    "    \n",
    "    if policy == \"olr\":\n",
    "        lr_manager = OneCycleLR(samples=train.shape[0], epochs=epochs, batch_size=batch_size,\n",
    "                            steps=len(train_iterator), max_lr=max_lr_olr,\n",
    "                            end_percentage=0.2, scale_percentage=None,\n",
    "                            maximum_momentum=None, minimum_momentum=None)\n",
    "        callbacks=[lr_manager]\n",
    "    else:\n",
    "        callbacks = [LearningRateScheduler(lr_shed_fn)] if lr_shed_fn is not None else []\n",
    "    \n",
    "    if policy in [\"olr\",\"sgd\"]:\n",
    "        optimizer = SGD(lr=lr, decay=0, momentum=0.9, nesterov=True)\n",
    "    else:\n",
    "        optimizer = Adam(lr=lr)\n",
    "    \n",
    "    model.compile(loss=[\"categorical_crossentropy\"],\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[\"acc\"],)\n",
    "    # print(model.summary())\n",
    "    \n",
    "    if val is None:\n",
    "        train_history = model.fit_generator(train_iterator,\n",
    "                        steps_per_epoch=len(train_iterator), \n",
    "                        epochs=epochs, verbose=1,\n",
    "                        callbacks=callbacks,)\n",
    "    else:\n",
    "        validation_iterator = MakeIter(val,extract_values,batch_size)\n",
    "        metrics = DataGenMetrics(train = MakeIter(train_copy,extract_values,batch_size),\n",
    "                             val = MakeIter(val,extract_values,batch_size))\n",
    "        callbacks.append(metrics)\n",
    "        train_history = model.fit_generator(train_iterator,\n",
    "                        steps_per_epoch=len(train_iterator), \n",
    "                        validation_data = validation_iterator, \n",
    "                        validation_steps = len(validation_iterator),\n",
    "                        epochs=epochs, verbose=1,\n",
    "                        callbacks=callbacks,)\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_test_wrapper(X_Y):\n",
    "    return X_Y[0]\n",
    "    \n",
    "def extract_values(x):\n",
    "    x1 = np.stack(x[\"full_txt_encoded\"].values, axis=0)\n",
    "    x2 = np.stack(x[\"full_txt_mask\"].values, axis=0)\n",
    "    x3 = np.stack(x[\"full_txt_mask_gaussian\"].values, axis=0)\n",
    "    x4 = np.stack(x[\"context_txt_encoded\"].values, axis=0)\n",
    "    x5 = x[[\"occurences_count\",\"review_length\",\"review_length_by_occurences_count\"]].values\n",
    "    train_phase = False\n",
    "    if 'ohe_labels' in x.columns:\n",
    "        train_phase = True\n",
    "        y = np.stack(x['ohe_labels'].values,axis=0)\n",
    "    \n",
    "    if train_phase:\n",
    "        x1,x2,x3 = batch_cutout( x1,x2,x3,p=cutout_proba, min_words=min_cutout, max_words=max_cutout,)\n",
    "        x4 = batch_cutout(x4, p=cutout_proba, min_words=min_cutout, max_words=max_cutout,)[0]\n",
    "    if train_phase:\n",
    "        return [x1,x2,x3,x4,x5],y\n",
    "    else:\n",
    "        return [x1,x2,x3,x4,x5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T17:53:06.206297Z",
     "start_time": "2019-07-24T17:41:15.272092Z"
    }
   },
   "outputs": [],
   "source": [
    "def looped_random_validation(df,model_fn, training_fn,extract_values,\n",
    "                             epochs=None, batch_size=None,\n",
    "                            policy=None,lr=0.01,max_lr_olr=0.005, lr_shed_fn=lambda e,lr:0.95*lr):\n",
    "    \n",
    "    \n",
    "    assert policy is not None\n",
    "    assert epochs is not None\n",
    "    assert batch_size is not None\n",
    "    model = model_fn()\n",
    "\n",
    "    np.random.seed(8*17 + 3*21+np.random.randint(8754))\n",
    "    df_train, df_val = train_test_split(df,test_size=0.3)\n",
    "    training_fn(model,df_train,df_val,extract_values,\n",
    "               epochs=epochs, batch_size=batch_size, policy=policy,lr=lr,max_lr_olr=max_lr_olr, lr_shed_fn=lr_shed_fn)\n",
    "\n",
    "    y_train_preds = model.predict(extract_test_wrapper(extract_values(df_train)))\n",
    "    y_train_preds = np.argmax(y_train_preds, axis=1)\n",
    "    _ = show_results(df_train['sentiment'], y_train_preds)\n",
    "\n",
    "    \n",
    "    y_val_preds = model.predict(extract_test_wrapper(extract_values(df_val)))\n",
    "    y_val_preds = np.argmax(y_val_preds, axis=1)\n",
    "    acc,f1_test = show_results(df_val['sentiment'], y_val_preds)\n",
    "    \n",
    "    print(classification_report(df_val['sentiment'], y_val_preds))\n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "      \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    full_txt_input = Input(shape=(full_txt_maxlen,), dtype='int32')\n",
    "    full_txt_mask = Input(shape=(full_txt_maxlen,), dtype='float32')\n",
    "    full_txt_gaussian = Input(shape=(full_txt_maxlen,), dtype='float32')\n",
    "    context_txt_input = Input(shape=(context_txt_maxlen,), dtype='int32')\n",
    "    \n",
    "    numeric_inputs = Input(shape=(3,),name=\"numeric_input\", dtype='float32')\n",
    "    \n",
    "    fc_layer_width = conv_embedded_params[\"fc_layer_width\"]\n",
    "    full_text_conv_layer_width = conv_embedded_params[\"full_text_conv_layer_width\"]\n",
    "    context_conv_layer_width = conv_embedded_params[\"context_conv_layer_width\"]\n",
    "    fc_layer_depth = conv_embedded_params[\"fc_layer_depth\"]\n",
    "    \n",
    "    \n",
    "    full_txt_embedding = Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=full_txt_maxlen,\n",
    "                    name=\"full_txt_embedding\",)\n",
    "    context_txt_embedding = Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=context_txt_maxlen,\n",
    "                    name=\"context_txt_maxlen\",)\n",
    "    \n",
    "    full_txt_embedding_out = full_txt_embedding(full_txt_input)\n",
    "    \n",
    "    context_txt_embedding_out = context_txt_embedding(context_txt_input) # 25 channels\n",
    "    \n",
    "    full_txt_embedding_out = Dropout(0.25)(full_txt_embedding_out)\n",
    "    context_txt_embedding_out = Dropout(0.25)(context_txt_embedding_out)\n",
    "    \n",
    "    # https://stackoverflow.com/questions/53849829/element-wise-multiplication-with-keras\n",
    "    # print(K.int_shape(full_txt_gaussian),K.int_shape(full_txt_embedding_out))\n",
    "    \n",
    "    gaussian_mask_out = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([full_txt_embedding_out, full_txt_gaussian])\n",
    "    full_txt_gaussian_out = Reshape((full_txt_maxlen,1))(full_txt_gaussian) \n",
    "    full_txt_mask_out = Reshape((full_txt_maxlen,1))(full_txt_mask) \n",
    "    \n",
    "    \n",
    "    fc_input1 = GlobalAveragePooling1D()(gaussian_mask_out)\n",
    "    fc_input1 = fc_layer(fc_input1,fc_layer_width,)\n",
    "    \n",
    "    full_txt_embedding_masked = concatenate([full_txt_mask_out,full_txt_embedding_out,full_txt_gaussian_out]) # 27 channels now\n",
    "    \n",
    "    \n",
    "    \n",
    "    fc_input2 = conv_pipe(context_txt_embedding_out, context_conv_layer_width)\n",
    "    fc_input3 = conv_pipe(full_txt_embedding_masked, full_text_conv_layer_width)\n",
    "    \n",
    "    fc_input1 = Dropout(0.2)(fc_input1)\n",
    "    fc_input2 = Dropout(0.2)(fc_input2)\n",
    "    fc_input3 = Dropout(0.2)(fc_input3)\n",
    "    \n",
    "    numerics = fc_layer(numeric_inputs,8,)\n",
    "    numerics = fc_layer(numerics,8,)\n",
    "    \n",
    "    fc_input = concatenate([numerics,fc_input1,fc_input2,fc_input3])\n",
    "    \n",
    "    for i in range(fc_layer_depth-1):\n",
    "        fc_input = fc_layer(fc_input,fc_layer_width)\n",
    "    \n",
    "    fc_out = fc_layer(fc_input,int(fc_layer_width/2),bn=False)\n",
    "    fc_out = Dense(3)(fc_out)\n",
    "    out = Activation(\"sigmoid\")(fc_out) # vs sigmoid\n",
    "    \n",
    "    model = Model(inputs=[full_txt_input,full_txt_mask,full_txt_gaussian,context_txt_input,numeric_inputs], \n",
    "                  outputs=[out])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5486662"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'full_text_conv_layer_width': 8,\n",
       " 'context_conv_layer_width': 6,\n",
       " 'fc_layer_width': 80,\n",
       " 'fc_layer_depth': 2,\n",
       " 'training_policy': {'epochs': 20,\n",
       "  'batch_size': 64,\n",
       "  'policy': 'adam',\n",
       "  'lr': 0.002,\n",
       "  'max_lr_olr': 0.005,\n",
       "  'lr_shed_fn': <function params.<lambda>(e, lr)>}}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Resampling, Size =  (3695, 18) \n",
      " 2    2692\n",
      "1     585\n",
      "0     418\n",
      "Name: sentiment, dtype: int64\n",
      "After Resampling =  (8543, 18) \n",
      " 0    2926\n",
      "1    2925\n",
      "2    2692\n",
      "Name: sentiment, dtype: int64\n",
      "Epoch 1/20\n",
      "134/134 [==============================] - 106s 791ms/step - loss: 1.0824 - acc: 0.4142 - val_loss: 1.0284 - val_acc: 0.4686\n",
      "Validation F1 Score = 0.3800, Train F1 Score = 0.4174\n",
      "Epoch 2/20\n",
      "134/134 [==============================] - 44s 327ms/step - loss: 0.9510 - acc: 0.5523 - val_loss: 0.9717 - val_acc: 0.5769\n",
      "Validation F1 Score = 0.4920, Train F1 Score = 0.5834\n",
      "Epoch 3/20\n",
      "134/134 [==============================] - 42s 310ms/step - loss: 0.7762 - acc: 0.6713 - val_loss: 0.9404 - val_acc: 0.5870\n",
      "Validation F1 Score = 0.5002, Train F1 Score = 0.6593\n",
      "Epoch 4/20\n",
      "134/134 [==============================] - 40s 296ms/step - loss: 0.6247 - acc: 0.7483 - val_loss: 1.0380 - val_acc: 0.6013\n",
      "Validation F1 Score = 0.5039, Train F1 Score = 0.7176\n",
      "Epoch 5/20\n",
      "134/134 [==============================] - 41s 305ms/step - loss: 0.5051 - acc: 0.8053 - val_loss: 1.1103 - val_acc: 0.5759\n",
      "Validation F1 Score = 0.4931, Train F1 Score = 0.7463\n",
      "Epoch 6/20\n",
      "134/134 [==============================] - 42s 311ms/step - loss: 0.4241 - acc: 0.8405 - val_loss: 1.1641 - val_acc: 0.6005\n",
      "Validation F1 Score = 0.4963, Train F1 Score = 0.7931\n",
      "Epoch 7/20\n",
      "134/134 [==============================] - 43s 319ms/step - loss: 0.3849 - acc: 0.8523 - val_loss: 1.0626 - val_acc: 0.6109\n",
      "Validation F1 Score = 0.5037, Train F1 Score = 0.8075\n",
      "Epoch 8/20\n",
      "134/134 [==============================] - 44s 325ms/step - loss: 0.3350 - acc: 0.8730 - val_loss: 1.0522 - val_acc: 0.6608\n",
      "Validation F1 Score = 0.5220, Train F1 Score = 0.8742\n",
      "Epoch 9/20\n",
      "134/134 [==============================] - 43s 323ms/step - loss: 0.3102 - acc: 0.8858 - val_loss: 1.0645 - val_acc: 0.6756\n",
      "Validation F1 Score = 0.5403, Train F1 Score = 0.8864\n",
      "Epoch 10/20\n",
      "134/134 [==============================] - 40s 300ms/step - loss: 0.2813 - acc: 0.9039 - val_loss: 1.0926 - val_acc: 0.6812\n",
      "Validation F1 Score = 0.5270, Train F1 Score = 0.9014\n",
      "Epoch 11/20\n",
      "129/134 [===========================>..] - ETA: 1s - loss: 0.2644 - acc: 0.9072"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "conv_embedded_params\n",
    "looped_random_validation(df,build_model,train_generator_model_with_validation,extract_values,\n",
    "                        **conv_embedded_params[\"training_policy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Accuracy = 0.89, Macro F1 = 0.86\n",
    "Accuracy = 0.65, Macro F1 = 0.52\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.30      0.41      0.35       199\n",
    "           1       0.41      0.51      0.45       248\n",
    "           2       0.82      0.72      0.77      1137\n",
    "\n",
    "   micro avg       0.65      0.65      0.65      1584\n",
    "   macro avg       0.51      0.55      0.52      1584\n",
    "weighted avg       0.69      0.65      0.66      1584\n",
    "```\n",
    "\n",
    "```\n",
    "Accuracy = 0.92, Macro F1 = 0.90\n",
    "Accuracy = 0.66, Macro F1 = 0.39\n",
    "Accuracy = 0.67, Macro F1 = 0.51\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "366/366 [==============================] - 41s 112ms/step - loss: 1.1686 - acc: 0.3845\n",
      " - lr: 0.01525 \n",
      "Epoch 2/2\n",
      "366/366 [==============================] - 32s 86ms/step - loss: 1.1251 - acc: 0.4472\n",
      " - lr: 0.00002 \n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "train_generator_model_with_validation(model,df,None,extract_values,\n",
    "                   **conv_embedded_params[\"training_policy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_test_preds = model.predict(extract_values(df_test))\n",
    "y_test_preds = np.argmax(y_test_preds, axis=1)\n",
    "assert len(y_test_preds)==df_test.shape[0]\n",
    "df_test['sentiment'] = y_test_preds\n",
    "df_test[['unique_hash','sentiment']].to_csv(\"submission.csv\",index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M1 variant: Only surrounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_values_only_surrounding(x):\n",
    "    x4 = np.stack(x[\"context_txt_encoded\"].values, axis=0)\n",
    "    x5 = x[[\"occurences_count\",\"review_length\",\"review_length_by_occurences_count\"]].values\n",
    "    train_phase = False\n",
    "    if 'ohe_labels' in x.columns:\n",
    "        train_phase = True\n",
    "        y = np.stack(x['ohe_labels'].values,axis=0)\n",
    "    if train_phase:\n",
    "        x4 = batch_cutout(x4, p=cutout_proba, min_words=min_cutout, max_words=max_cutout,)[0]\n",
    "    \n",
    "    if train_phase:\n",
    "        return [x4,x5],y\n",
    "    else:\n",
    "        return [x4,x5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_m1v():\n",
    "    context_txt_input = Input(shape=(context_txt_maxlen,), dtype='int32')\n",
    "    \n",
    "    numeric_inputs = Input(shape=(3,),name=\"numeric_input\", dtype='float32')\n",
    "    \n",
    "    fc_layer_width = conv_embedded_params_m1v1[\"fc_layer_width\"]\n",
    "    context_conv_layer_width = conv_embedded_params_m1v1[\"context_conv_layer_width\"]\n",
    "    fc_layer_depth = conv_embedded_params_m1v1[\"fc_layer_depth\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    context_txt_embedding = Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=context_txt_maxlen,\n",
    "                    name=\"context_txt_maxlen\",)\n",
    "    \n",
    "    context_txt_embedding_out = context_txt_embedding(context_txt_input) # 25 channels\n",
    "    context_txt_embedding_out = Dropout(0.2)(context_txt_embedding_out)\n",
    "    \n",
    "    # https://stackoverflow.com/questions/53849829/element-wise-multiplication-with-keras\n",
    "    # print(K.int_shape(full_txt_gaussian),K.int_shape(full_txt_embedding_out))\n",
    "    \n",
    "    fc_input2 = conv_pipe(context_txt_embedding_out, context_conv_layer_width)\n",
    "    fc_input2 = Dropout(0.2)(fc_input2)\n",
    "    numerics = fc_layer(numeric_inputs,8,)\n",
    "    numerics = fc_layer(numerics,8,)\n",
    "    \n",
    "    fc_input = concatenate([numerics,fc_input2])\n",
    "    \n",
    "    for i in range(fc_layer_depth-1):\n",
    "        fc_input = fc_layer(fc_input,fc_layer_width)\n",
    "    \n",
    "    fc_out = fc_layer(fc_input,int(fc_layer_width/2),bn=False)\n",
    "    fc_out = Dense(3)(fc_out)\n",
    "    out = Activation(\"softmax\")(fc_out)\n",
    "    \n",
    "    model = Model(inputs=[context_txt_input,numeric_inputs], \n",
    "                  outputs=[out])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2106"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'context_conv_layer_width': 8,\n",
       " 'fc_layer_width': 80,\n",
       " 'fc_layer_depth': 2,\n",
       " 'training_policy': {'epochs': 20,\n",
       "  'batch_size': 64,\n",
       "  'policy': 'adam',\n",
       "  'lr': 0.002,\n",
       "  'max_lr_olr': 0.005,\n",
       "  'lr_shed_fn': <function params.<lambda>(e, lr)>}}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Resampling, Size =  (3695, 18) \n",
      " 2    2659\n",
      "1     605\n",
      "0     431\n",
      "Name: sentiment, dtype: int64\n",
      "After Resampling =  (8701, 18) \n",
      " 1    3025\n",
      "0    3017\n",
      "2    2659\n",
      "Name: sentiment, dtype: int64\n",
      "Epoch 1/20\n",
      "136/136 [==============================] - 47s 342ms/step - loss: 1.1243 - acc: 0.3822 - val_loss: 1.1457 - val_acc: 0.4658\n",
      "Validation F1 Score = 0.3735, Train F1 Score = 0.3819\n",
      "Epoch 2/20\n",
      "136/136 [==============================] - 14s 106ms/step - loss: 1.0255 - acc: 0.4750 - val_loss: 1.1882 - val_acc: 0.4133\n",
      "Validation F1 Score = 0.3789, Train F1 Score = 0.4346\n",
      "Epoch 3/20\n",
      "136/136 [==============================] - 14s 106ms/step - loss: 0.9207 - acc: 0.5615 - val_loss: 1.1083 - val_acc: 0.4438\n",
      "Validation F1 Score = 0.4051, Train F1 Score = 0.4965\n",
      "Epoch 4/20\n",
      "136/136 [==============================] - 14s 105ms/step - loss: 0.7911 - acc: 0.6540 - val_loss: 1.0532 - val_acc: 0.4699\n",
      "Validation F1 Score = 0.4304, Train F1 Score = 0.5729\n",
      "Epoch 5/20\n",
      "136/136 [==============================] - 14s 106ms/step - loss: 0.6591 - acc: 0.7277 - val_loss: 0.8448 - val_acc: 0.6092\n",
      "Validation F1 Score = 0.5081, Train F1 Score = 0.7032\n",
      "Epoch 6/20\n",
      "136/136 [==============================] - 14s 105ms/step - loss: 0.5693 - acc: 0.7735 - val_loss: 0.8543 - val_acc: 0.6100\n",
      "Validation F1 Score = 0.4890, Train F1 Score = 0.7644\n",
      "Epoch 7/20\n",
      "136/136 [==============================] - 15s 107ms/step - loss: 0.5070 - acc: 0.8018 - val_loss: 0.8504 - val_acc: 0.6506\n",
      "Validation F1 Score = 0.5099, Train F1 Score = 0.8150\n",
      "Epoch 8/20\n",
      "136/136 [==============================] - 14s 104ms/step - loss: 0.4518 - acc: 0.8192 - val_loss: 0.8984 - val_acc: 0.6392\n",
      "Validation F1 Score = 0.5201, Train F1 Score = 0.8022\n",
      "Epoch 9/20\n",
      "136/136 [==============================] - 14s 104ms/step - loss: 0.4188 - acc: 0.8378 - val_loss: 0.9426 - val_acc: 0.6290\n",
      "Validation F1 Score = 0.4969, Train F1 Score = 0.8299\n",
      "Epoch 10/20\n",
      "136/136 [==============================] - 14s 105ms/step - loss: 0.4031 - acc: 0.8487 - val_loss: 0.9216 - val_acc: 0.6685\n",
      "Validation F1 Score = 0.5226, Train F1 Score = 0.8417\n",
      "Epoch 11/20\n",
      "136/136 [==============================] - 14s 105ms/step - loss: 0.3553 - acc: 0.8667 - val_loss: 0.9115 - val_acc: 0.6804\n",
      "Validation F1 Score = 0.5322, Train F1 Score = 0.8668\n",
      "Epoch 12/20\n",
      "136/136 [==============================] - 14s 105ms/step - loss: 0.3616 - acc: 0.8640 - val_loss: 1.0438 - val_acc: 0.6044\n",
      "Validation F1 Score = 0.5165, Train F1 Score = 0.8399\n",
      "Epoch 13/20\n",
      "136/136 [==============================] - 14s 106ms/step - loss: 0.3312 - acc: 0.8773 - val_loss: 1.0254 - val_acc: 0.6611\n",
      "Validation F1 Score = 0.5164, Train F1 Score = 0.8815\n",
      "Epoch 14/20\n",
      "136/136 [==============================] - 14s 106ms/step - loss: 0.3261 - acc: 0.8837 - val_loss: 0.9807 - val_acc: 0.6951\n",
      "Validation F1 Score = 0.5240, Train F1 Score = 0.8924\n",
      "Epoch 15/20\n",
      "136/136 [==============================] - 14s 105ms/step - loss: 0.3137 - acc: 0.8878 - val_loss: 1.0403 - val_acc: 0.6559\n",
      "Validation F1 Score = 0.5054, Train F1 Score = 0.9033\n",
      "Epoch 16/20\n",
      "136/136 [==============================] - 14s 105ms/step - loss: 0.3238 - acc: 0.8823 - val_loss: 1.0866 - val_acc: 0.6472\n",
      "Validation F1 Score = 0.5186, Train F1 Score = 0.8587\n",
      "Epoch 17/20\n",
      "136/136 [==============================] - 14s 106ms/step - loss: 0.2879 - acc: 0.8986 - val_loss: 0.9642 - val_acc: 0.6701\n",
      "Validation F1 Score = 0.5300, Train F1 Score = 0.8731\n",
      "Epoch 18/20\n",
      "136/136 [==============================] - 14s 106ms/step - loss: 0.2669 - acc: 0.9058 - val_loss: 1.0947 - val_acc: 0.6464\n",
      "Validation F1 Score = 0.5147, Train F1 Score = 0.8952\n",
      "Epoch 19/20\n",
      "136/136 [==============================] - 14s 106ms/step - loss: 0.3371 - acc: 0.8789 - val_loss: 1.0541 - val_acc: 0.6562\n",
      "Validation F1 Score = 0.5162, Train F1 Score = 0.8972\n",
      "Epoch 20/20\n",
      "136/136 [==============================] - 14s 104ms/step - loss: 0.2554 - acc: 0.9126 - val_loss: 1.0413 - val_acc: 0.6657\n",
      "Validation F1 Score = 0.5205, Train F1 Score = 0.8896\n",
      "Accuracy = 0.93, Macro F1 = 0.91\n",
      "Accuracy = 0.67, Macro F1 = 0.52\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.40      0.31       186\n",
      "           1       0.50      0.42      0.46       232\n",
      "           2       0.81      0.76      0.78      1166\n",
      "\n",
      "   micro avg       0.67      0.67      0.67      1584\n",
      "   macro avg       0.52      0.53      0.52      1584\n",
      "weighted avg       0.70      0.67      0.68      1584\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "conv_embedded_params_m1v1\n",
    "looped_random_validation(df,build_model_m1v,train_generator_model_with_validation,extract_values_only_surrounding,\n",
    "                        **conv_embedded_params_m1v1[\"training_policy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Accuracy = 0.91, Macro F1 = 0.88\n",
    "Accuracy = 0.66, Macro F1 = 0.53\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.30      0.42      0.35       189\n",
    "           1       0.42      0.52      0.46       252\n",
    "           2       0.82      0.73      0.77      1143\n",
    "\n",
    "   micro avg       0.66      0.66      0.66      1584\n",
    "   macro avg       0.51      0.55      0.53      1584\n",
    "weighted avg       0.70      0.66      0.67      1584\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "Accuracy = 0.87, Macro F1 = 0.82\n",
    "Accuracy = 0.67, Macro F1 = 0.49\n",
    "Accuracy = 0.69, Macro F1 = 0.53\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.36      0.35      0.36       113\n",
    "           1       0.42      0.44      0.43       179\n",
    "           2       0.80      0.80      0.80       764\n",
    "\n",
    "   micro avg       0.69      0.69      0.69      1056\n",
    "   macro avg       0.53      0.53      0.53      1056\n",
    "weighted avg       0.69      0.69      0.69      1056\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "model = build_model_m1v()\n",
    "train_generator_model_with_validation(model,df,None,extract_values_only_surrounding,\n",
    "                   **conv_embedded_params_m1v1[\"training_policy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_test_preds = model.predict(extract_values_only_surrounding(df_test))\n",
    "y_test_preds = np.argmax(y_test_preds, axis=1)\n",
    "assert len(y_test_preds)==df_test.shape[0]\n",
    "df_test['sentiment'] = y_test_preds\n",
    "df_test[['unique_hash','sentiment']].to_csv(\"submission.csv\",index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2: CNN Model - No Padding and Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5279, 14)"
      ]
     },
     "execution_count": 613,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2924, 12)"
      ]
     },
     "execution_count": 613,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_np = read_csv(\"train_non_padded.csv\")\n",
    "df_test_np = read_csv(\"test_non_padded.csv\")\n",
    "\n",
    "df_np.shape\n",
    "df_test_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_embeddings = PreTrainedEmbeddingsTransformer(fb_model,size=fasttext_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_values_pretrained(x):\n",
    "    x1 = x[\"full_txt\"].values\n",
    "    x2 = x[\"full_txt_mask\"].values\n",
    "    x3 = x[\"full_txt_mask_gaussian\"].values\n",
    "    x4 = x[\"context_txt\"].values\n",
    "    x5 = x[[\"occurences_count\",\"review_length\",\"review_length_by_occurences_count\"]].values\n",
    "    train_phase = False\n",
    "    if 'ohe_labels' in x.columns:\n",
    "        train_phase = True\n",
    "        y = np.stack(x['ohe_labels'].values,axis=0)\n",
    "\n",
    "    \n",
    "\n",
    "    full_txt_maxlen = 0\n",
    "    for x in x1:\n",
    "        full_txt_maxlen = max(full_txt_maxlen,len(x))\n",
    "        \n",
    "    context_txt_maxlen = 0\n",
    "    \n",
    "    x1 = list(pad_text_sequences(x1, maxlen=full_txt_maxlen,jobs=1))\n",
    "    x2 = list(pad_text_sequences(x2, maxlen=full_txt_maxlen,empty=0.0,jobs=1))\n",
    "    x3 = list(pad_text_sequences(x3, maxlen=full_txt_maxlen,empty=0.0,jobs=1))\n",
    "    x4 = list(pad_text_sequences(x4, maxlen=maxlen,jobs=1))\n",
    "    \n",
    "    x1 = np.array(x1)\n",
    "    x4 = np.array(x4)\n",
    "    \n",
    "    if train_phase:\n",
    "        x1,x2,x3 = batch_cutout( x1,x2,x3,p=cutout_proba, min_words=min_cutout, max_words=max_cutout,)\n",
    "        \n",
    "        x4 = batch_cutout(x4, p=cutout_proba, min_words=min_cutout, max_words=max_cutout,)[0]\n",
    "    \n",
    "    \n",
    "    x4 = trained_embeddings.transform(x4)\n",
    "    x1 = trained_embeddings.transform(x1)\n",
    "    \n",
    "    # Batch padding!!\n",
    "    \n",
    "    \n",
    "    x1 = np.stack(x1, axis=0)\n",
    "    x4 = np.stack(x4, axis=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    x2 = np.asarray([np.asarray(x).reshape((len(x),1)) for x in x2])\n",
    "    x3 = np.asarray([np.asarray(x).reshape((len(x),1)) for x in x3])\n",
    "    x2 = np.stack(x2, axis=0)\n",
    "    x3 = np.stack(x3, axis=0)\n",
    "    # print(x1.shape,x2.shape,x3.shape,x4.shape)\n",
    "    if train_phase:\n",
    "        return [x1,x2,x3,x4,x5],y\n",
    "    else:\n",
    "        return [x1,x2,x3,x4,x5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 750,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = df_np.sample(1000)\n",
    "_ = extract_values_pretrained(sample)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_no_embedding():\n",
    "    full_txt_input = Input(shape=(None,fasttext_dims), dtype='float32')\n",
    "    full_txt_mask = Input(shape=(None,1), dtype='float32')\n",
    "    full_txt_gaussian = Input(shape=(None,1), dtype='float32')\n",
    "    \n",
    "    context_txt_input = Input(shape=(None,fasttext_dims), dtype='float32')\n",
    "    \n",
    "    numeric_inputs = Input(shape=(3,),name=\"numeric_input\", dtype='float32')\n",
    "    \n",
    "    fc_layer_width = conv_non_embedded_params[\"fc_layer_width\"]\n",
    "    full_text_conv_layer_width = conv_non_embedded_params[\"full_text_conv_layer_width\"]\n",
    "    context_conv_layer_width = conv_non_embedded_params[\"context_conv_layer_width\"]\n",
    "    fc_layer_depth = conv_non_embedded_params[\"fc_layer_depth\"]\n",
    "    \n",
    "    \n",
    "    full_txt_embedding_out = full_txt_input\n",
    "    context_txt_embedding_out = context_txt_input # 25 channels\n",
    "    \n",
    "    full_txt_embedding_out = Dropout(0.1)(full_txt_embedding_out)\n",
    "    context_txt_embedding_out = Dropout(0.1)(context_txt_embedding_out)\n",
    "    print(K.int_shape(full_txt_embedding_out))\n",
    "    \n",
    "    # https://stackoverflow.com/questions/53849829/element-wise-multiplication-with-keras\n",
    "    gaussian_mask_out = Lambda(lambda x: x[0] * x[1])([full_txt_embedding_out, full_txt_gaussian])\n",
    "    print(K.int_shape(gaussian_mask_out))\n",
    "    full_txt_gaussian_out = full_txt_gaussian\n",
    "    full_txt_mask_out = full_txt_mask\n",
    "    print(K.int_shape(full_txt_mask_out))\n",
    "    \n",
    "    fc_input1 = GlobalAveragePooling1D()(gaussian_mask_out)\n",
    "    fc_input1 = fc_layer(fc_input1,fc_layer_width,)\n",
    "    \n",
    "    full_txt_embedding_masked = concatenate([full_txt_mask_out,full_txt_embedding_out,full_txt_gaussian_out]) # 27 channels now\n",
    "    print(K.int_shape(full_txt_embedding_masked))\n",
    "    \n",
    "    fc_input2 = conv_pipe(context_txt_embedding_out, context_conv_layer_width)\n",
    "    fc_input3 = conv_pipe(full_txt_embedding_masked, full_text_conv_layer_width)\n",
    "    \n",
    "    fc_input2 = Dropout(0.1)(fc_input2)\n",
    "    numerics = fc_layer(numeric_inputs,8,)\n",
    "    numerics = fc_layer(numerics,8,)\n",
    "    \n",
    "    fc_input = concatenate([numerics,fc_input1,fc_input2,fc_input3])\n",
    "    \n",
    "    for i in range(fc_layer_depth-1):\n",
    "        fc_input = fc_layer(fc_input,fc_layer_width)\n",
    "    \n",
    "    fc_out = fc_layer(fc_input,int(fc_layer_width/2),bn=False)\n",
    "    fc_out = Dense(3)(fc_out)\n",
    "    out = Activation(\"softmax\")(fc_out)\n",
    "    \n",
    "    model = Model(inputs=[full_txt_input,full_txt_mask,full_txt_gaussian,context_txt_input,numeric_inputs], \n",
    "                  outputs=[out])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 752,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'full_text_conv_layer_width': 6, 'context_conv_layer_width': 4, 'fc_layer_width': 80, 'fc_layer_depth': 2, 'training_policy': {'epochs': 10, 'batch_size': 32, 'policy': 'adam', 'lr': 0.005, 'max_lr_olr': 0.005, 'lr_shed_fn': <function <lambda> at 0x7fae86602d90>}}\n",
      "(None, None, 300)\n",
      "(None, None, 300)\n",
      "(None, None, 1)\n",
      "(None, None, 302)\n",
      "Before Resampling = \n",
      " 2    2744\n",
      "1     619\n",
      "0     437\n",
      "Name: sentiment, dtype: int64\n",
      "After Resampling = \n",
      " 1    3095\n",
      "0    3059\n",
      "2    2744\n",
      "Name: sentiment, dtype: int64\n",
      "Epoch 1/10\n",
      "  8/279 [..............................] - ETA: 4:28:18 - loss: 1.3198 - acc: 0.3203"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "print(conv_non_embedded_params)\n",
    "looped_random_validation(df_np,build_model_no_embedding,train_generator_model_with_validation,extract_values_pretrained,\n",
    "                        **conv_non_embedded_params[\"training_policy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model_no_embedding()\n",
    "train_generator_model_with_validation(model,df_np,None,extract_values_pretrained,\n",
    "                   **conv_non_embedded_params[\"training_policy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = MakeIter(df_test_np,extract_values,batch_size)\n",
    "y_test_preds = model.predict_generator(iterator,steps=len(iterator))\n",
    "y_test_preds = np.argmax(y_test_preds, axis=1)\n",
    "df_test_np['sentiment'] = y_test_preds\n",
    "df_test_np[['unique_hash','sentiment']].to_csv(\"submission.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M3: XGB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xgb = read_csv(\"train_non_padded.csv\")\n",
    "df_test_xgb = read_csv(\"test_non_padded.csv\")\n",
    "\n",
    "\n",
    "doc2vec = PreTrainedDocEmbeddingsTransformer(fb_model,start=0,end=300)\n",
    "\n",
    "def append_doc2vec_columns(df,column):\n",
    "    results = doc2vec.transform(df[column])\n",
    "    text_df = pd.DataFrame(list(map(list, results)))\n",
    "    text_df.columns = [column +\"_doc2vec_\"+ str(i) for i in range(0, doc2vec.size)]\n",
    "    text_df.index = df.index\n",
    "    df[list(text_df.columns)] = text_df\n",
    "    return df\n",
    "    \n",
    "\n",
    "df_xgb = append_doc2vec_columns(df_xgb,column=\"full_txt\")\n",
    "df_xgb = append_doc2vec_columns(df_xgb,column=\"context_txt\")\n",
    "df_test_xgb = append_doc2vec_columns(df_test_xgb,column=\"full_txt\")\n",
    "df_test_xgb = append_doc2vec_columns(df_test_xgb,column=\"context_txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xgb_features = df_utils.get_specific_cols(df_xgb,prefix=\"full_txt_doc2vec_\") + df_utils.get_specific_cols(df_xgb,prefix=\"context_txt_doc2vec_\") +[\"occurences_count\",\"review_length\",\"review_length_by_occurences_count\"]\n",
    "xgb_features = df_utils.get_specific_cols(df_xgb,prefix=\"context_txt_doc2vec_\") +[\"occurences_count\",\"review_length\",\"review_length_by_occurences_count\"]\n",
    "target = \"ohe_labels\"\n",
    "len(xgb_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgb(model,train):\n",
    "    train = resample_input_for_training(train)\n",
    "    y = np.asarray([x for x in train[target].values])\n",
    "    train = train[xgb_features]\n",
    "    model.fit(train,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_builder():\n",
    "    # objective=\"binary:logitraw\",\n",
    "    model = OneVsRestClassifier(XGBClassifier(n_jobs=-1, max_depth=6,scale_pos_weight=0.5,\n",
    "                                              booster='dart',n_estimators=100,learning_rate=0.1,\n",
    "                                              tree_method=\"exact\",\n",
    "                                             subsample=0.75,gamma=8,colsample_bylevel=0.5,colsample_bytree=0.5,\n",
    "                                             alpha=0.4,num_parallel_tree=2))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def looped_xgb_validation(looping=5):\n",
    "    model = xgb_builder()\n",
    "    fscores = []\n",
    "    for i in range(looping):\n",
    "        np.random.seed(i*17 + 5*13+np.random.randint(10000))\n",
    "        df_train, df_val = train_test_split(df_xgb,test_size=0.2)\n",
    "        \n",
    "        train_xgb(model,df_train)\n",
    "\n",
    "        y_train_preds = model.predict_proba(df_train[xgb_features])\n",
    "#         print(y_train_preds)\n",
    "        y_train_preds = np.argmax(y_train_preds, axis=1)\n",
    "        \n",
    "        _ = show_results(df_train['sentiment'], y_train_preds)\n",
    "\n",
    "        \n",
    "        y_val_preds = model.predict_proba(df_val[xgb_features])\n",
    "        y_val_preds = np.argmax(y_val_preds, axis=1)\n",
    "        acc,f1_test = show_results(df_val['sentiment'], y_val_preds)\n",
    "        model = xgb_builder()\n",
    "        fscores.append(f1_test)\n",
    "    print(\"Mean Fscores = \",np.mean(fscores))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.92, Macro F1 = 0.90\n",
      "Accuracy = 0.63, Macro F1 = 0.46\n",
      "Accuracy = 0.92, Macro F1 = 0.90\n",
      "Accuracy = 0.63, Macro F1 = 0.48\n",
      "Accuracy = 0.91, Macro F1 = 0.89\n",
      "Accuracy = 0.64, Macro F1 = 0.47\n",
      "Accuracy = 0.92, Macro F1 = 0.90\n",
      "Accuracy = 0.63, Macro F1 = 0.48\n",
      "Accuracy = 0.92, Macro F1 = 0.90\n",
      "Accuracy = 0.65, Macro F1 = 0.50\n",
      "Mean Fscores =  0.4761089456811837\n"
     ]
    }
   ],
   "source": [
    "looped_xgb_validation()\n",
    "\n",
    "# max_depth = 6 0.47385244733641335\n",
    "# max_depth = 4 0.4758742505101491\n",
    "\n",
    "# 0.47720480008127925"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Methods**\n",
    "- Use Keras Layer to generate vector representation of words\n",
    "- Use FB Fasttext with GAP, GMP of both full_txt and context_txt\n",
    "- No Padding Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M5: BiDirectional LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model():\n",
    "    full_txt_input = Input(shape=(full_txt_maxlen,), dtype='int32')\n",
    "    full_txt_mask = Input(shape=(full_txt_maxlen,), dtype='float32')\n",
    "    full_txt_gaussian = Input(shape=(full_txt_maxlen,), dtype='float32')\n",
    "    context_txt_input = Input(shape=(context_txt_maxlen,), dtype='int32')\n",
    "    \n",
    "    numeric_inputs = Input(shape=(3,),name=\"numeric_input\", dtype='float32')\n",
    "    \n",
    "    fc_layer_width = lstm_embedded_params[\"fc_layer_width\"]\n",
    "    lstm_context_units = lstm_embedded_params[\"lstm_context_units\"]\n",
    "    lstm_full_text_units = lstm_embedded_params[\"lstm_full_text_units\"]\n",
    "    fc_layer_depth = lstm_embedded_params[\"fc_layer_depth\"]\n",
    "    \n",
    "    \n",
    "    full_txt_embedding = Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=full_txt_maxlen,\n",
    "                    name=\"full_txt_embedding\",)\n",
    "    context_txt_embedding = Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=context_txt_maxlen,\n",
    "                    name=\"context_txt_maxlen\",)\n",
    "    \n",
    "    full_txt_embedding_out = full_txt_embedding(full_txt_input)\n",
    "    context_txt_embedding_out = context_txt_embedding(context_txt_input) # 25 channels\n",
    "    context_txt_embedding_out = Dropout(0.3)(context_txt_embedding_out)\n",
    "    full_txt_embedding_out = Dropout(0.3)(full_txt_embedding_out)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # https://stackoverflow.com/questions/53849829/element-wise-multiplication-with-keras\n",
    "    # print(K.int_shape(full_txt_gaussian),K.int_shape(full_txt_embedding_out))\n",
    "    \n",
    "    gaussian_mask_out = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([full_txt_embedding_out, full_txt_gaussian])\n",
    "    full_txt_gaussian_out = Reshape((full_txt_maxlen,1))(full_txt_gaussian) \n",
    "    full_txt_mask_out = Reshape((full_txt_maxlen,1))(full_txt_mask) \n",
    "    \n",
    "    \n",
    "    fc_input1 = GlobalAveragePooling1D()(gaussian_mask_out)\n",
    "    fc_input1 = fc_layer(fc_input1,fc_layer_width,)\n",
    "    \n",
    "    full_txt_embedding_masked = concatenate([full_txt_mask_out,full_txt_embedding_out,full_txt_gaussian_out]) # 27 channels now\n",
    "    # print(K.int_shape(full_txt_embedding_masked))\n",
    "    fc_input2 = Bidirectional(LSTM(lstm_context_units,dropout=0.1, return_sequences=True))(context_txt_embedding_out)\n",
    "    fc_input3 = Bidirectional(LSTM(lstm_full_text_units, dropout=0.1, return_sequences=True))(full_txt_embedding_masked)\n",
    "    \n",
    "    fc_input2 = Bidirectional(LSTM(lstm_context_units,dropout=0.1))(fc_input2)\n",
    "    fc_input3 = Bidirectional(LSTM(lstm_full_text_units, dropout=0.1))(fc_input3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    fc_input1 = Dropout(0.1)(fc_input1)\n",
    "    fc_input2 = Dropout(0.1)(fc_input2)\n",
    "    numerics = fc_layer(numeric_inputs,8,)\n",
    "    numerics = fc_layer(numerics,8,)\n",
    "    \n",
    "    fc_input = concatenate([numerics,fc_input1,fc_input2,fc_input3])\n",
    "    \n",
    "    for i in range(fc_layer_depth-1):\n",
    "        fc_input = fc_layer(fc_input,fc_layer_width)\n",
    "    \n",
    "    fc_out = fc_layer(fc_input,int(fc_layer_width/2),bn=False)\n",
    "    fc_out = Dense(3)(fc_out)\n",
    "    out = Activation(\"softmax\")(fc_out)\n",
    "    \n",
    "    model = Model(inputs=[full_txt_input,full_txt_mask,full_txt_gaussian,context_txt_input,numeric_inputs], \n",
    "                  outputs=[out])\n",
    "    \n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11460"
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lstm_full_text_units': 64, 'lstm_context_units': 32, 'fc_layer_width': 32, 'fc_layer_depth': 3, 'training_policy': {'epochs': 20, 'batch_size': 64, 'policy': 'adam', 'lr': 0.001, 'max_lr_olr': 0.005, 'lr_shed_fn': <function <lambda> at 0x7faf35185488>}}\n",
      "Before Resampling = \n",
      " 2    2754\n",
      "1     606\n",
      "0     440\n",
      "Name: sentiment, dtype: int64\n",
      "After Resampling = \n",
      " 2    2754\n",
      "1    2424\n",
      "0    2200\n",
      "Name: sentiment, dtype: int64\n",
      "Epoch 1/20\n",
      "116/116 [==============================] - 750s 6s/step - loss: 1.1535 - acc: 0.3411 - val_loss: 1.1128 - val_acc: 0.3036\n",
      "Validation F1 Score = 0.2434, Train F1 Score = 0.2547\n",
      "Epoch 2/20\n",
      "116/116 [==============================] - 444s 4s/step - loss: 1.1059 - acc: 0.3810 - val_loss: 1.0714 - val_acc: 0.4460\n",
      "Validation F1 Score = 0.2895, Train F1 Score = 0.3072\n",
      "Epoch 3/20\n",
      "116/116 [==============================] - 443s 4s/step - loss: 1.0878 - acc: 0.3805 - val_loss: 1.1055 - val_acc: 0.3696\n",
      "Validation F1 Score = 0.2598, Train F1 Score = 0.3303\n",
      "Epoch 4/20\n",
      "116/116 [==============================] - 443s 4s/step - loss: 1.0805 - acc: 0.4181 - val_loss: 1.0623 - val_acc: 0.4062\n",
      "Validation F1 Score = 0.2965, Train F1 Score = 0.2747\n",
      "Epoch 5/20\n",
      "116/116 [==============================] - 441s 4s/step - loss: 1.0842 - acc: 0.3969 - val_loss: 0.9799 - val_acc: 0.6496\n",
      "Validation F1 Score = 0.3619, Train F1 Score = 0.4095\n",
      "Epoch 6/20\n",
      "116/116 [==============================] - 444s 4s/step - loss: 1.0666 - acc: 0.4321 - val_loss: 1.0623 - val_acc: 0.4649\n",
      "Validation F1 Score = 0.2844, Train F1 Score = 0.3682\n",
      "Epoch 7/20\n",
      "116/116 [==============================] - 447s 4s/step - loss: 1.0716 - acc: 0.4253 - val_loss: 0.9811 - val_acc: 0.6384\n",
      "Validation F1 Score = 0.4059, Train F1 Score = 0.4216\n",
      "Epoch 8/20\n",
      "116/116 [==============================] - 452s 4s/step - loss: 1.0549 - acc: 0.4418 - val_loss: 0.9970 - val_acc: 0.5579\n",
      "Validation F1 Score = 0.4316, Train F1 Score = 0.3652\n",
      "Epoch 9/20\n",
      "116/116 [==============================] - 452s 4s/step - loss: 1.0542 - acc: 0.4250 - val_loss: 0.8606 - val_acc: 0.6985\n",
      "Validation F1 Score = 0.3813, Train F1 Score = 0.3662\n",
      "Epoch 10/20\n",
      "116/116 [==============================] - 452s 4s/step - loss: 1.0272 - acc: 0.4763 - val_loss: 0.9497 - val_acc: 0.4955\n",
      "Validation F1 Score = 0.3796, Train F1 Score = 0.4507\n",
      "Epoch 11/20\n",
      "116/116 [==============================] - 451s 4s/step - loss: 1.0108 - acc: 0.4877 - val_loss: 0.9664 - val_acc: 0.4795\n",
      "Validation F1 Score = 0.3794, Train F1 Score = 0.5018\n",
      "Epoch 12/20\n",
      "116/116 [==============================] - 450s 4s/step - loss: 1.0066 - acc: 0.4860 - val_loss: 0.9746 - val_acc: 0.5190\n",
      "Validation F1 Score = 0.4265, Train F1 Score = 0.4595\n",
      "Epoch 13/20\n",
      "116/116 [==============================] - 446s 4s/step - loss: 0.9725 - acc: 0.5192 - val_loss: 1.0075 - val_acc: 0.4335\n",
      "Validation F1 Score = 0.3333, Train F1 Score = 0.4914\n",
      "Epoch 14/20\n",
      "116/116 [==============================] - 448s 4s/step - loss: 0.9525 - acc: 0.5388 - val_loss: 0.9828 - val_acc: 0.5000\n",
      "Validation F1 Score = 0.3982, Train F1 Score = 0.4725\n",
      "Epoch 15/20\n",
      "116/116 [==============================] - 449s 4s/step - loss: 0.9383 - acc: 0.5496 - val_loss: 0.9242 - val_acc: 0.5389\n",
      "Validation F1 Score = 0.4141, Train F1 Score = 0.5504\n",
      "Epoch 16/20\n",
      "116/116 [==============================] - 448s 4s/step - loss: 0.9129 - acc: 0.5695 - val_loss: 0.8984 - val_acc: 0.5330\n",
      "Validation F1 Score = 0.3774, Train F1 Score = 0.5538\n",
      "Epoch 17/20\n",
      "116/116 [==============================] - 447s 4s/step - loss: 0.8982 - acc: 0.5780 - val_loss: 0.9637 - val_acc: 0.5211\n",
      "Validation F1 Score = 0.4115, Train F1 Score = 0.6196\n",
      "Epoch 18/20\n",
      "116/116 [==============================] - 444s 4s/step - loss: 0.8888 - acc: 0.5913 - val_loss: 0.9382 - val_acc: 0.5099\n",
      "Validation F1 Score = 0.4216, Train F1 Score = 0.5647\n",
      "Epoch 19/20\n",
      "116/116 [==============================] - 442s 4s/step - loss: 0.8718 - acc: 0.5993 - val_loss: 0.8130 - val_acc: 0.5714\n",
      "Validation F1 Score = 0.4196, Train F1 Score = 0.6456\n",
      "Epoch 20/20\n",
      "116/116 [==============================] - 444s 4s/step - loss: 0.8436 - acc: 0.6177 - val_loss: 1.1596 - val_acc: 0.3714\n",
      "Validation F1 Score = 0.2894, Train F1 Score = 0.5274\n",
      "Accuracy = 0.57, Macro F1 = 0.54\n",
      "Accuracy = 0.43, Macro F1 = 0.37\n",
      "Accuracy = 0.48, Macro F1 = 0.43\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.43      0.26       119\n",
      "           1       0.31      0.74      0.44       176\n",
      "           2       0.88      0.43      0.58       761\n",
      "\n",
      "   micro avg       0.48      0.48      0.48      1056\n",
      "   macro avg       0.46      0.53      0.43      1056\n",
      "weighted avg       0.71      0.48      0.52      1056\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "print(lstm_embedded_params)\n",
    "looped_random_validation(build_lstm_model,train_generator_model_with_validation,extract_values,\n",
    "                        **lstm_embedded_params[\"training_policy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M6: LSTM No Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_no_embedding():\n",
    "    full_txt_input = Input(shape=(None,fasttext_dims), dtype='float32')\n",
    "    full_txt_mask = Input(shape=(None,1), dtype='float32')\n",
    "    full_txt_gaussian = Input(shape=(None,1), dtype='float32')\n",
    "    \n",
    "    \n",
    "    context_txt_input = Input(shape=(None,fasttext_dims), dtype='float32')\n",
    "    \n",
    "\n",
    "    numeric_inputs = Input(shape=(3,),name=\"numeric_input\", dtype='float32')\n",
    "\n",
    "    fc_layer_width = lstm_non_embedded_params[\"fc_layer_width\"]\n",
    "    lstm_context_units = lstm_non_embedded_params[\"lstm_context_units\"]\n",
    "    lstm_full_text_units = lstm_non_embedded_params[\"lstm_full_text_units\"]\n",
    "    fc_layer_depth = lstm_non_embedded_params[\"fc_layer_depth\"]\n",
    "    \n",
    "    \n",
    "    full_txt_embedding_out = full_txt_input\n",
    "    context_txt_embedding_out = context_txt_input # 25 channels\n",
    "    \n",
    "    \n",
    "    # https://stackoverflow.com/questions/53849829/element-wise-multiplication-with-keras\n",
    "    # print(K.int_shape(full_txt_gaussian),K.int_shape(full_txt_embedding_out))\n",
    "    # gaussian_mask_out = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([full_txt_embedding_out, full_txt_gaussian])\n",
    "    gaussian_mask_out = Lambda(lambda x: x[0] * x[1])([full_txt_embedding_out, full_txt_gaussian])\n",
    "    print(K.int_shape(gaussian_mask_out))\n",
    "    full_txt_gaussian_out = full_txt_gaussian\n",
    "    full_txt_mask_out = full_txt_mask\n",
    "    # full_txt_gaussian_out = Reshape((-1,1))(full_txt_gaussian) \n",
    "    # full_txt_mask_out = Reshape((-1,1))(full_txt_mask) \n",
    "    \n",
    "    print(K.int_shape(gaussian_mask_out))\n",
    "    fc_input1 = GlobalAveragePooling1D()(gaussian_mask_out)\n",
    "    fc_input1 = fc_layer(fc_input1,fc_layer_width,)\n",
    "    \n",
    "    full_txt_embedding_masked = concatenate([full_txt_mask_out,full_txt_embedding_out,full_txt_gaussian_out]) # 27 channels now\n",
    "    print(K.int_shape(full_txt_embedding_out),K.int_shape(full_txt_embedding_masked))\n",
    "    \n",
    "    fc_input2 = Bidirectional(LSTM(lstm_context_units))(context_txt_embedding_out)\n",
    "    fc_input3 = Bidirectional(LSTM(lstm_full_text_units))(full_txt_embedding_masked)\n",
    "    \n",
    "    fc_input1 = Dropout(0.1)(fc_input1)\n",
    "    fc_input2 = Dropout(0.1)(fc_input2)\n",
    "    fc_input3 = Dropout(0.1)(fc_input3)\n",
    "    numerics = fc_layer(numeric_inputs,8,)\n",
    "    numerics = fc_layer(numerics,8,)\n",
    "    \n",
    "    \n",
    "    fc_input = concatenate([numerics,fc_input1,fc_input2,fc_input3])\n",
    "    \n",
    "    for i in range(fc_layer_depth-1):\n",
    "        fc_input = fc_layer(fc_input,fc_layer_width)\n",
    "    \n",
    "    fc_out = fc_layer(fc_input,int(fc_layer_width/2),bn=False)\n",
    "    fc_out = Dense(3)(fc_out)\n",
    "    out = Activation(\"softmax\")(fc_out)\n",
    "    \n",
    "    model = Model(inputs=[full_txt_input,full_txt_mask,full_txt_gaussian,context_txt_input,numeric_inputs], \n",
    "                  outputs=[out])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "print(lstm_non_embedded_params)\n",
    "looped_random_validation(df_np,build_lstm_no_embedding,train_generator_model_with_validation,extract_values_pretrained,\n",
    "                        **lstm_non_embedded_params[\"training_policy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xgb = read_csv(\"train_non_padded.csv\")\n",
    "df_test_xgb = read_csv(\"test_non_padded.csv\")\n",
    "\n",
    "\n",
    "doc2vec = PreTrainedDocEmbeddingsTransformer(fb_model,start=0,end=300)\n",
    "\n",
    "def append_doc2vec_columns(df,column):\n",
    "    results = doc2vec.transform(df[column])\n",
    "    text_df = pd.DataFrame(list(map(list, results)))\n",
    "    text_df.columns = [column +\"_doc2vec_\"+ str(i) for i in range(0, doc2vec.size)]\n",
    "    text_df.index = df.index\n",
    "    df[list(text_df.columns)] = text_df\n",
    "    return df\n",
    "        \n",
    "\n",
    "df_xgb = append_doc2vec_columns(df_xgb,column=\"full_txt\")\n",
    "df_xgb = append_doc2vec_columns(df_xgb,column=\"context_txt\")\n",
    "df_test_xgb = append_doc2vec_columns(df_test_xgb,column=\"full_txt\")\n",
    "df_test_xgb = append_doc2vec_columns(df_test_xgb,column=\"context_txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_features = df_utils.get_specific_cols(df_xgb,prefix=\"context_txt_doc2vec_\") +[\"occurences_count\",\"review_length\",\"review_length_by_occurences_count\"]\n",
    "target = \"ohe_labels\"\n",
    "len(xgb_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgb(model_1,model_2,train):\n",
    "    train = resample_input_for_training(train)\n",
    "    y_s = np.asarray([x[2]==1 for x in train[target].values])\n",
    "    y_n = np.asarray([x[1]==1 for x in train[target].values])\n",
    "    \n",
    "    y_i = np.asarray([x[1]==1 or x[0]==1 for x in train[target].values])\n",
    "    train = train[xgb_features]\n",
    "    model_1.fit(train,y_s.astype(int))\n",
    "    model_2.fit(train[y_i],y_n[y_i])\n",
    "    \n",
    "def predict_xgb(model_1,model_2,df):\n",
    "    y_s = model_1.predict(df)\n",
    "    y_s[y_s==1] = 2\n",
    "    \n",
    "    y_2 = model_2.predict(df)\n",
    "    y_s[y_s!=2] = y_2[y_s!=2]\n",
    "    return y_s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_builder(scale_pos_weight):\n",
    "    # objective=\"binary:logitraw\",\n",
    "    model = XGBClassifier(n_jobs=-1, max_depth=8, scale_pos_weight=scale_pos_weight,\n",
    "                                              booster='dart',n_estimators=100,learning_rate=0.1,\n",
    "                                              tree_method=\"exact\",\n",
    "                                             subsample=0.75,gamma=8,colsample_bylevel=0.5,colsample_bytree=0.5,\n",
    "                                             alpha=0.4,num_parallel_tree=2)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "def looped_xgb_validation(looping=5):\n",
    "    \n",
    "    fscores = []\n",
    "    for i in range(looping):\n",
    "        m1 = xgb_builder(1.7)\n",
    "        m2 = xgb_builder(0.7)\n",
    "        np.random.seed(i*17 + 5*13+np.random.randint(10000))\n",
    "        df_train, df_val = train_test_split(df_xgb,test_size=0.2)\n",
    "        \n",
    "        train_xgb(m1,m2,df_train)\n",
    "\n",
    "        y_train_preds = predict_xgb(m1,m2,df_train[xgb_features])\n",
    "#         print(y_train_preds)\n",
    "        # y_train_preds = np.argmax(y_train_preds, axis=1)\n",
    "        \n",
    "        _ = show_results(df_train['sentiment'], y_train_preds)\n",
    "\n",
    "        \n",
    "        y_val_preds = predict_xgb(m1,m2,df_val[xgb_features])\n",
    "        # y_val_preds = np.argmax(y_val_preds, axis=1)\n",
    "        acc,f1_test = show_results(df_val['sentiment'], y_val_preds)\n",
    "        fscores.append(f1_test)\n",
    "        print(classification_report(df_val['sentiment'], y_val_preds))\n",
    "    print(\"Mean Fscores = \",np.mean(fscores))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.99, Macro F1 = 0.99\n",
      "Accuracy = 0.68, Macro F1 = 0.47\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.23      0.24       120\n",
      "           1       0.37      0.34      0.35       165\n",
      "           2       0.80      0.82      0.81       771\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      1056\n",
      "   macro avg       0.47      0.46      0.47      1056\n",
      "weighted avg       0.67      0.68      0.67      1056\n",
      "\n",
      "Accuracy = 0.99, Macro F1 = 0.99\n",
      "Accuracy = 0.66, Macro F1 = 0.45\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.17      0.21       143\n",
      "           1       0.33      0.35      0.34       156\n",
      "           2       0.77      0.82      0.79       757\n",
      "\n",
      "   micro avg       0.66      0.66      0.66      1056\n",
      "   macro avg       0.46      0.44      0.45      1056\n",
      "weighted avg       0.64      0.66      0.65      1056\n",
      "\n",
      "Accuracy = 0.99, Macro F1 = 0.99\n",
      "Accuracy = 0.69, Macro F1 = 0.47\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.19      0.23       128\n",
      "           1       0.37      0.38      0.38       158\n",
      "           2       0.79      0.83      0.81       770\n",
      "\n",
      "   micro avg       0.69      0.69      0.69      1056\n",
      "   macro avg       0.48      0.47      0.47      1056\n",
      "weighted avg       0.67      0.69      0.67      1056\n",
      "\n",
      "Accuracy = 0.99, Macro F1 = 0.99\n",
      "Accuracy = 0.68, Macro F1 = 0.48\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.17      0.19       132\n",
      "           1       0.43      0.44      0.44       176\n",
      "           2       0.79      0.82      0.81       748\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      1056\n",
      "   macro avg       0.49      0.48      0.48      1056\n",
      "weighted avg       0.66      0.68      0.67      1056\n",
      "\n",
      "Accuracy = 0.99, Macro F1 = 0.99\n",
      "Accuracy = 0.67, Macro F1 = 0.46\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.20      0.22       127\n",
      "           1       0.39      0.31      0.35       176\n",
      "           2       0.77      0.83      0.80       753\n",
      "\n",
      "   micro avg       0.67      0.67      0.67      1056\n",
      "   macro avg       0.47      0.45      0.46      1056\n",
      "weighted avg       0.64      0.67      0.65      1056\n",
      "\n",
      "Mean Fscores =  0.46404006598577724\n"
     ]
    }
   ],
   "source": [
    "looped_xgb_validation(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6201"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3345 + 2856"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "320px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
