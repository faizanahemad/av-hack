{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next Steps**\n",
    "\n",
    "- Try with and without stopword removal\n",
    "\n",
    "- Reduce incoming to FC layer\n",
    "    - Decomplicate network\n",
    "\n",
    "- verify cutout not happening in predictions, try more aggresive cutout\n",
    "\n",
    "- Softmax to Sigmoid\n",
    "\n",
    "- Wide / Deep / Max Pool\n",
    "\n",
    "- class_weight = {0: 1.,1: 50., 2: 2.}\n",
    "\n",
    "- optimal thresholding (Optimal thresholding for F1)\n",
    "\n",
    "- Remove top 2% misclassified\n",
    "\n",
    "\n",
    "\n",
    "**Aces**\n",
    "\n",
    "- dropout and regularization\n",
    "- OLR+SGD+momentum or LR Scheduling\n",
    "- Architecture and Multiple Receptive Field\n",
    "    - Go Wide\n",
    "- HyperParams from Params Section\n",
    "- Try with and without removing hypen `-`\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features**\n",
    "- Average Rating of Drug\n",
    "- don't do stop words\n",
    "- Use Drug Column through embedding layer as well to get Drug Vector.\n",
    "- Run Fasttext/Glove on this corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    "\n",
    "# The GPU id to use, usually either \"0\" or \"1\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\";  \n",
    " \n",
    "# Do other imports now...\n",
    "import keras\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available()\n",
    "\n",
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.backend.tensorflow_backend import clear_session\n",
    "from keras.backend.tensorflow_backend import get_session\n",
    "import tensorflow\n",
    "\n",
    "# Reset Keras Session\n",
    "def reset_keras():\n",
    "    sess = get_session()\n",
    "    clear_session()\n",
    "    sess.close()\n",
    "    sess = get_session()\n",
    "\n",
    "    try:\n",
    "        del classifier # this is from global space - change this as you need\n",
    "        del model\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    print(gc.collect()) # if it's done something you should see a number being outputted\n",
    "\n",
    "    # use the same config as you used to create the session\n",
    "    config = tensorflow.ConfigProto(log_device_placement=True, allow_soft_placement=True,)\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "    config.gpu_options.visible_device_list = \"0\"\n",
    "    set_session(tensorflow.Session(config=config))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "reset_keras()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T16:20:07.179065Z",
     "start_time": "2019-07-24T16:20:01.945144Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'data_science_utils.models' from '/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/data_science_utils/models/__init__.py'>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<module 'data_science_utils.vision.keras' from '/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/data_science_utils/vision/keras/__init__.py'>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<module 'lib' from '/home/ec2-user/SageMaker/lib.py'>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<module 'params' from '/home/ec2-user/SageMaker/params.py'>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<module 'data_gen' from '/home/ec2-user/SageMaker/data_gen.py'>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Reshape, Multiply\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GRU\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "%matplotlib inline\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, DepthwiseConv2D, Conv2D, SeparableConv2D, MaxPooling1D, AveragePooling1D\n",
    "from keras.layers import Input, concatenate, LeakyReLU\n",
    "import gensim.downloader as api\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Activation, Flatten, Dense, Dropout, SpatialDropout1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD, Nadam, Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "from keras.regularizers import l2\n",
    "%config InlineBackend.figure_format='retina'\n",
    "from keras_contrib.callbacks import CyclicLR\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "\n",
    "from time import time\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import missingno as msno\n",
    "import re\n",
    "from joblib import Parallel, delayed\n",
    "from data_science_utils import dataframe as df_utils\n",
    "from data_science_utils import models as model_utils\n",
    "from data_science_utils import plots as plot_utils\n",
    "from data_science_utils.dataframe import column as column_utils\n",
    "from data_science_utils import misc as misc\n",
    "from data_science_utils import preprocessing as pp_utils\n",
    "from data_science_utils import nlp as nlp_utils\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from data_science_utils.dataframe import get_specific_cols\n",
    "from random import sample\n",
    "\n",
    "import more_itertools\n",
    "from more_itertools import flatten\n",
    "import ast\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import gc\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd())\n",
    "from importlib import reload\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "reload(model_utils)\n",
    "\n",
    "import data_science_utils.vision.keras\n",
    "reload(data_science_utils.vision.keras)\n",
    "from data_science_utils.vision.keras import *\n",
    "\n",
    "from data_science_utils.models import mean_absolute_percentage_error\n",
    "from data_science_utils.models import median_absolute_percentage_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "from more_itertools import flatten\n",
    "import dill\n",
    "from collections import Counter\n",
    "import operator\n",
    "from gensim.models import FastText\n",
    "import itertools\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.utils import tokenize\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "\n",
    "import lib\n",
    "reload(lib)\n",
    "from lib import *\n",
    "\n",
    "import params\n",
    "reload(params)\n",
    "from params import *\n",
    "\n",
    "import data_gen\n",
    "reload(data_gen)\n",
    "from data_gen import *\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from keras.regularizers import l2\n",
    "from keras.regularizers import L1L2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = api.load(\"glove-twitter-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fname = os.getcwd()+\"/fasttext.model\"\n",
    "from gensim.models import FastText\n",
    "fb_model = FastText.load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T17:35:47.727614Z",
     "start_time": "2019-07-24T17:35:47.363819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5279, 22)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2924, 20)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = read_csv(\"train_padded.csv\")\n",
    "df_test = read_csv(\"test_padded.csv\")\n",
    "\n",
    "\n",
    "df.shape\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T17:39:49.289992Z",
     "start_time": "2019-07-24T17:39:48.120167Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25548"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "18769"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Exclusive Distinct words =  4267\n",
      "Total vocab size =  29815\n"
     ]
    }
   ],
   "source": [
    "# How many words in test set are not in train set\n",
    "\n",
    "train_words = set(list(more_itertools.flatten(df.full_txt.values)))\n",
    "test_words = set(list(more_itertools.flatten(df_test.full_txt.values)))\n",
    "all_words = set(list(more_itertools.flatten(df.full_txt.values))+list(more_itertools.flatten(df_test.full_txt.values)))\n",
    "len(train_words)\n",
    "len(test_words)\n",
    "test_exlusive_words = test_words-train_words\n",
    "\n",
    "print(\"Test Set Exclusive Distinct words = \",len(test_exlusive_words))\n",
    "\n",
    "print(\"Total vocab size = \",len(all_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcess for M1 and M5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8203/8203 [00:00<00:00, 23072.89it/s]\n",
      " 19%|█▉        | 1020/5279 [00:00<00:00, 10195.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words before Min frequency filtering 29815\n",
      "Total Words after Min frequency filtering 11859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5279/5279 [00:00<00:00, 10034.96it/s]\n",
      "100%|██████████| 2924/2924 [00:00<00:00, 9277.64it/s]\n",
      "100%|██████████| 5279/5279 [00:00<00:00, 11034.55it/s]\n",
      "100%|██████████| 2924/2924 [00:00<00:00, 7111.22it/s] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "le_train,le_transform, le = get_text_le(vocab_size=vocab_size, min_count=min_count)\n",
    "_ = le_train(list(df.full_txt.values)+list(df_test.full_txt.values))\n",
    "\n",
    "df[\"full_txt_encoded\"] = le_transform(df.full_txt.values)\n",
    "df_test[\"full_txt_encoded\"] = le_transform(df_test.full_txt.values)\n",
    "\n",
    "df[\"context_txt_encoded\"] = le_transform(df.context_txt.values)\n",
    "df_test[\"context_txt_encoded\"] = le_transform(df_test.context_txt.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T17:39:54.125578Z",
     "start_time": "2019-07-24T17:39:54.094245Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.rand(vocab_size, embedding_dims)*0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T17:39:54.935701Z",
     "start_time": "2019-07-24T17:39:54.128958Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient 7\n",
      "[-1.3836      0.71107    -1.83200002  0.65379     0.74203002 -0.1154\n",
      "  0.47209999 -0.58218998  0.034721    0.17285    -0.51932001  0.26039001\n",
      " -2.91829991  0.67479998  0.38541001  0.23262     0.39111     0.49118999\n",
      " -0.043787   -1.05869997  0.71371001 -0.11127    -1.02769995  0.13327\n",
      " -0.60834998]\n"
     ]
    }
   ],
   "source": [
    "not_printed=True\n",
    "for w in le[\"wd\"].keys():\n",
    "    try:\n",
    "        embedding_matrix[le[\"wd\"][w]] = glove.wv[w][:embedding_dims]\n",
    "        if not_printed:\n",
    "            not_printed=False\n",
    "            print(w,le[\"wd\"][w])\n",
    "            print(embedding_matrix[le[\"wd\"][w]])\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## One Cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "# Code is ported from https://github.com/fastai/fastai\n",
    "class OneCycleLR(Callback):\n",
    "    def __init__(self,\n",
    "                 epochs,\n",
    "                 batch_size,\n",
    "                 samples,\n",
    "                 steps,\n",
    "                 max_lr,\n",
    "                 end_percentage=0.1,\n",
    "                 scale=100,\n",
    "                 maximum_momentum=0.95,\n",
    "                 minimum_momentum=0.85,\n",
    "                 verbose=True):\n",
    "        \"\"\" This callback implements a cyclical learning rate policy (CLR).\n",
    "        This is a special case of Cyclic Learning Rates, where we have only 1 cycle.\n",
    "        After the completion of 1 cycle, the learning rate will decrease rapidly to\n",
    "        100th its initial lowest value.\n",
    "        # Arguments:\n",
    "            max_lr: Float. Initial learning rate. This also sets the\n",
    "                starting learning rate (which will be 10x smaller than\n",
    "                this), and will increase to this value during the first cycle.\n",
    "            end_percentage: Float. The percentage of all the epochs of training\n",
    "                that will be dedicated to sharply decreasing the learning\n",
    "                rate after the completion of 1 cycle. Must be between 0 and 1.\n",
    "            scale_percentage: Float or None. If float, must be between 0 and 1.\n",
    "                If None, it will compute the scale_percentage automatically\n",
    "                based on the `end_percentage`.\n",
    "            maximum_momentum: Optional. Sets the maximum momentum (initial)\n",
    "                value, which gradually drops to its lowest value in half-cycle,\n",
    "                then gradually increases again to stay constant at this max value.\n",
    "                Can only be used with SGD Optimizer.\n",
    "            minimum_momentum: Optional. Sets the minimum momentum at the end of\n",
    "                the half-cycle. Can only be used with SGD Optimizer.\n",
    "            verbose: Bool. Whether to print the current learning rate after every\n",
    "                epoch.\n",
    "        # Reference\n",
    "            - [A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, weight_decay, and weight decay](https://arxiv.org/abs/1803.09820)\n",
    "            - [Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates](https://arxiv.org/abs/1708.07120)\n",
    "        \"\"\"\n",
    "        super(OneCycleLR, self).__init__()\n",
    "\n",
    "        if end_percentage < 0. or end_percentage > 1.:\n",
    "            raise ValueError(\"`end_percentage` must be between 0 and 1\")\n",
    "\n",
    "\n",
    "        self.initial_lr = max_lr\n",
    "        self.end_percentage = end_percentage\n",
    "        self.scale = scale\n",
    "        self.max_momentum = maximum_momentum\n",
    "        self.min_momentum = minimum_momentum\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if self.max_momentum is not None and self.min_momentum is not None:\n",
    "            self._update_momentum = True\n",
    "        else:\n",
    "            self._update_momentum = False\n",
    "\n",
    "        self.clr_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.samples = samples\n",
    "        self.steps = steps\n",
    "        self.num_iterations = None\n",
    "        self.mid_cycle_id = None\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"\n",
    "        Reset the callback.\n",
    "        \"\"\"\n",
    "        self.clr_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "    def compute_lr(self):\n",
    "        \"\"\"\n",
    "        Compute the learning rate based on which phase of the cycle it is in.\n",
    "        - If in the first half of training, the learning rate gradually increases.\n",
    "        - If in the second half of training, the learning rate gradually decreases.\n",
    "        - If in the final `end_percentage` portion of training, the learning rate\n",
    "            is quickly reduced to near 100th of the original min learning rate.\n",
    "        # Returns:\n",
    "            the new learning rate\n",
    "        \"\"\"\n",
    "        if self.clr_iterations > 2 * self.mid_cycle_id:\n",
    "            current_percentage = (self.clr_iterations - 2 * self.mid_cycle_id)\n",
    "            current_percentage /= float((self.num_iterations - 2 * self.mid_cycle_id))\n",
    "            new_lr = self.initial_lr * (1. + (current_percentage * (1. - 100.) / 100.)) / self.scale\n",
    "\n",
    "        elif self.clr_iterations > self.mid_cycle_id:\n",
    "            current_percentage = 1. - (\n",
    "                    self.clr_iterations - self.mid_cycle_id) / self.mid_cycle_id\n",
    "            new_lr = self.initial_lr * (1. + current_percentage * (self.scale - 1.)) / self.scale\n",
    "\n",
    "        else:\n",
    "            current_percentage = self.clr_iterations / self.mid_cycle_id\n",
    "            new_lr = self.initial_lr * (1. + current_percentage * (self.scale - 1.)) / self.scale\n",
    "\n",
    "        if self.clr_iterations == self.num_iterations:\n",
    "            self.clr_iterations = 0\n",
    "\n",
    "        return new_lr\n",
    "\n",
    "    def compute_momentum(self):\n",
    "        \"\"\"\n",
    "         Compute the momentum based on which phase of the cycle it is in.\n",
    "        - If in the first half of training, the momentum gradually decreases.\n",
    "        - If in the second half of training, the momentum gradually increases.\n",
    "        - If in the final `end_percentage` portion of training, the momentum value\n",
    "            is kept constant at the maximum initial value.\n",
    "        # Returns:\n",
    "            the new momentum value\n",
    "        \"\"\"\n",
    "        if self.clr_iterations > 2 * self.mid_cycle_id:\n",
    "            new_momentum = self.max_momentum\n",
    "\n",
    "        elif self.clr_iterations > self.mid_cycle_id:\n",
    "            current_percentage = 1. - ((self.clr_iterations - self.mid_cycle_id) / float(\n",
    "                self.mid_cycle_id))\n",
    "            new_momentum = self.max_momentum - current_percentage * (\n",
    "                    self.max_momentum - self.min_momentum)\n",
    "\n",
    "        else:\n",
    "            current_percentage = self.clr_iterations / float(self.mid_cycle_id)\n",
    "            new_momentum = self.max_momentum - current_percentage * (\n",
    "                    self.max_momentum - self.min_momentum)\n",
    "\n",
    "        return new_momentum\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.steps is not None:\n",
    "            self.num_iterations = self.epochs * self.steps\n",
    "        else:\n",
    "            if (self.samples % self.batch_size) == 0:\n",
    "                remainder = 0\n",
    "            else:\n",
    "                remainder = 1\n",
    "            self.num_iterations = (self.epochs + remainder) * self.samples // self.batch_size\n",
    "\n",
    "        self.mid_cycle_id = int(self.num_iterations * ((1. - self.end_percentage)) / float(2))\n",
    "\n",
    "        self._reset()\n",
    "        K.set_value(self.model.optimizer.lr, self.compute_lr())\n",
    "\n",
    "        if self._update_momentum:\n",
    "            if not hasattr(self.model.optimizer, 'momentum'):\n",
    "                raise ValueError(\"Momentum can be updated only on SGD optimizer !\")\n",
    "\n",
    "            new_momentum = self.compute_momentum()\n",
    "            K.set_value(self.model.optimizer.momentum, new_momentum)\n",
    "\n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "\n",
    "        self.clr_iterations += 1\n",
    "        new_lr = self.compute_lr()\n",
    "\n",
    "        self.history.setdefault('lr', []).append(\n",
    "            K.get_value(self.model.optimizer.lr))\n",
    "        K.set_value(self.model.optimizer.lr, new_lr)\n",
    "\n",
    "        if self._update_momentum:\n",
    "            if not hasattr(self.model.optimizer, 'momentum'):\n",
    "                raise ValueError(\"Momentum can be updated only on SGD optimizer !\")\n",
    "\n",
    "            new_momentum = self.compute_momentum()\n",
    "\n",
    "            self.history.setdefault('momentum', []).append(\n",
    "                K.get_value(self.model.optimizer.momentum))\n",
    "            K.set_value(self.model.optimizer.momentum, new_momentum)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.verbose:\n",
    "            if self._update_momentum:\n",
    "                print(\" - lr: %0.5f - momentum: %0.2f \" %\n",
    "                      (self.history['lr'][-1], self.history['momentum'][-1]))\n",
    "\n",
    "            else:\n",
    "                print(\" - lr: %0.5f \" % (self.history['lr'][-1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_block(x,layer_width,dropout=0.1):\n",
    "    x1 = conv_layer(x,n_kernels=16*layer_width,kernel_size=5,padding='same')\n",
    "    x2 = conv_layer(x1,n_kernels=16*layer_width,kernel_size=3,padding='same')\n",
    "    x = concatenate([x1,x2])\n",
    "    x3 = conv_layer(x,n_kernels=8*layer_width,kernel_size=3,padding='same')\n",
    "    \n",
    "    x = concatenate([x1,x2,x3])\n",
    "    x = transition_layer(x, n_kernels=16*layer_width, dropout=dropout)\n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T17:41:15.134250Z",
     "start_time": "2019-07-24T17:41:15.114251Z"
    }
   },
   "outputs": [],
   "source": [
    "def conv_pipe(x,layer_width,dropout=0):\n",
    "    \n",
    "    x1 = dense_block(x,layer_width)\n",
    "    xg0 = GlobalAveragePooling1D()(x1)\n",
    "    x2 = dense_block(x1,layer_width)\n",
    "    xg1 = GlobalAveragePooling1D()(x2)\n",
    "    \n",
    "    # x3 = MaxPooling1D()(x2)\n",
    "    x4 = dense_block(x2,layer_width)\n",
    "    xg2 = GlobalAveragePooling1D()(x4)\n",
    "    return concatenate([xg0,xg1, xg2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_pipe(x,layer_width,dropout=0):\n",
    "    \n",
    "    x1 = dense_block(x,int(1.25*layer_width))\n",
    "    xg0 = GlobalAveragePooling1D()(x1)\n",
    "    x2 = MaxPooling1D()(x1)\n",
    "    x2 = dense_block(x2,layer_width)\n",
    "    xg1 = GlobalAveragePooling1D()(x2)\n",
    "    \n",
    "    # x3 = MaxPooling1D()(x2)\n",
    "    return concatenate([xg0,xg1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T17:41:15.205637Z",
     "start_time": "2019-07-24T17:41:15.171080Z"
    }
   },
   "outputs": [],
   "source": [
    "def resample_input_for_training(x):\n",
    "    xs1 = x[x['sentiment']==0]\n",
    "    xs2 = x[x['sentiment']==1]\n",
    "    x = pd.concat([x,xs1.sample(frac=1),xs1.sample(frac=1),xs1.sample(frac=1),xs1.sample(frac=1),xs1.sample(frac=1),xs1.sample(frac=1),xs1.sample(frac=1)])\n",
    "    x = pd.concat([x,xs2.sample(frac=1),xs2.sample(frac=1),xs2.sample(frac=1),xs2.sample(frac=1),xs2.sample(frac=1)])\n",
    "    x = x.sample(frac=1)\n",
    "    \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator_model_with_validation(model,train,val=None,extract_values=None,weights=None,\n",
    "                                          epochs=None, batch_size=None,\n",
    "                                          policy=None,lr=0.01,max_lr_olr=0.005, \n",
    "                                          lr_shed_fn=lambda e,lr:0.95*lr,\n",
    "                                         df_test = None):\n",
    "    # policy olr+sgd | sgd+lrs | adam\n",
    "    train_copy = train.copy()\n",
    "    train = resample_input_for_training(train)\n",
    "    train_iterator = MakeIter(train,extract_values,batch_size)\n",
    "    print(\"Before Resampling, Size = \",train_copy.shape,\"\\n\",train_copy.sentiment.value_counts())\n",
    "    print(\"After Resampling = \",train.shape,\"\\n\",train.sentiment.value_counts())\n",
    "    checkpoint = ModelCheckpoint(\"model.hdf5\", monitor='acc', verbose=0, save_best_only=True, mode='max')\n",
    "    assert epochs is not None\n",
    "    assert policy is not None\n",
    "    assert batch_size is not None\n",
    "    assert weights is not None\n",
    "    \n",
    "    if policy == \"olr\":\n",
    "        lr_manager = OneCycleLR(samples=train.shape[0], epochs=epochs, batch_size=batch_size,\n",
    "                            steps=len(train_iterator), max_lr=max_lr_olr,\n",
    "                            end_percentage=0.2,\n",
    "                            maximum_momentum=None, minimum_momentum=None)\n",
    "        callbacks=[lr_manager]\n",
    "    else:\n",
    "        callbacks = [LearningRateScheduler(lr_decay)] if lr_shed_fn is not None else []\n",
    "    \n",
    "    if policy in [\"olr\",\"sgd\"]:\n",
    "        optimizer = SGD(lr=lr, decay=0, momentum=0.9, nesterov=True)\n",
    "    else:\n",
    "        optimizer = Adam(lr=lr)\n",
    "    \n",
    "    model.compile(loss=[\"categorical_crossentropy\"],\n",
    "                  optimizer=optimizer)\n",
    "    # print(model.summary())\n",
    "    print(\"Total Model Params = \",model.count_params())\n",
    "#     class_weight = {0: 6.2,1: 4.5, 2: 1.}\n",
    "#     class_weight = {0: 2,1: 1.5, 2: 1.}\n",
    "    if val is None:\n",
    "        metrics = DataGenMetrics(train = MakeIter(train_copy,extract_values,batch_size),\n",
    "                             val = None, weights = weights, df_test = df_test, extract_values = extract_values)\n",
    "        callbacks.append(metrics)\n",
    "        train_history = model.fit_generator(train_iterator,\n",
    "                        steps_per_epoch=len(train_iterator), \n",
    "                        epochs=epochs, verbose=1,\n",
    "                        callbacks=callbacks)\n",
    "        return metrics\n",
    "    else:\n",
    "        validation_iterator = MakeIter(val,extract_values,batch_size)\n",
    "        metrics = DataGenMetrics(train = MakeIter(train_copy,extract_values,batch_size),\n",
    "                             val = MakeIter(val,extract_values,batch_size), weights = weights)\n",
    "        callbacks.append(metrics)\n",
    "        train_history = model.fit_generator(train_iterator,\n",
    "                        steps_per_epoch=len(train_iterator), \n",
    "                        validation_data = validation_iterator, \n",
    "                        validation_steps = len(validation_iterator),\n",
    "                        epochs=epochs, verbose=2,\n",
    "                        callbacks=callbacks)\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T17:53:06.206297Z",
     "start_time": "2019-07-24T17:41:15.272092Z"
    }
   },
   "outputs": [],
   "source": [
    "def looped_random_validation(df,model_fn, training_fn,extract_values,weights=[1,1,1],\n",
    "                             epochs=None, batch_size=None,\n",
    "                            policy=None,lr=0.01,max_lr_olr=0.005, lr_shed_fn=lambda e,lr:0.95*lr):\n",
    "    \n",
    "    \n",
    "    assert policy is not None\n",
    "    assert epochs is not None\n",
    "    assert batch_size is not None\n",
    "    model = model_fn()\n",
    "\n",
    "    np.random.seed(8*91 + 3*19+np.random.randint(5617))\n",
    "    df_train, df_val = train_test_split(df,test_size=0.3)\n",
    "    training_fn(model,df_train,df_val,extract_values,weights,\n",
    "               epochs=epochs, batch_size=batch_size, policy=policy,lr=lr,max_lr_olr=max_lr_olr, lr_shed_fn=lr_shed_fn)\n",
    "\n",
    "    y_train_preds = model.predict(extract_test_wrapper(extract_values(df_train)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    y_train_preds_out = np.argmax(y_train_preds, axis=1)\n",
    "    _ = show_results(df_train['sentiment'], y_train_preds_out)\n",
    "\n",
    "    \n",
    "    y_val_preds = model.predict(extract_test_wrapper(extract_values(df_val)))\n",
    "    y_val_preds_out = np.argmax(y_val_preds, axis=1)\n",
    "    acc,f1_test = show_results(df_val['sentiment'], y_val_preds_out)\n",
    "    \n",
    "    find_best_caliberation(df_train['sentiment'],df_val['sentiment'],y_train_preds,y_val_preds)\n",
    "    \n",
    "    print(classification_report(df_val['sentiment'], y_val_preds_out))\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_test_wrapper(X_Y):\n",
    "    return X_Y[0]\n",
    "    \n",
    "def extract_values(x):\n",
    "    x1 = np.stack(x[\"full_txt_encoded\"].values, axis=0)\n",
    "    x2 = np.stack(x[\"full_txt_mask\"].values, axis=0)\n",
    "    x3 = np.stack(x[\"full_txt_mask_gaussian\"].values, axis=0)\n",
    "    x4 = np.stack(x[\"context_txt_encoded\"].values, axis=0)\n",
    "    x5 = x[[\"occurences_count\",\"review_length\",\"review_length_by_occurences_count\",\"first_occurence\",\"last_occurence\"]].values\n",
    "    \n",
    "    x6 = np.stack(x['word_sentiments'].values, axis=0)\n",
    "    x7 = np.stack(x['word_count'].values, axis=0)\n",
    "    \n",
    "    x8 = np.stack(x['word_sentiments_ctx'].values, axis=0)\n",
    "    x9 = np.stack(x['word_count_ctx'].values, axis=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # print(x1.shape,x2.shape,x3.shape,x4.shape,x5.shape)\n",
    "    train_phase = False\n",
    "    if 'ohe_labels' in x.columns:\n",
    "        train_phase = True\n",
    "        y = np.stack(x['ohe_labels'].values,axis=0)\n",
    "    \n",
    "    \n",
    "    if train_phase:\n",
    "        x1,x2,x3,x6,x7 = batch_cutout( x1,x2,x3,x6,x7, p=cutout_proba, min_words=min_cutout, max_words=max_cutout,)\n",
    "        x4,x8,x9 = batch_cutout(x4,x8,x9, p=cutout_proba, min_words=min_cutout, max_words=max_cutout,)\n",
    "    else:\n",
    "        print(\"Test Predictions - No cutout\")\n",
    "    # print(x1.shape,x2.shape,x3.shape,x4.shape,x5.shape)\n",
    "    if train_phase:\n",
    "        return [x1,x2,x3,x4,x5, x6, x7, x8, x9],y\n",
    "    else:\n",
    "        return [x1,x2,x3,x4,x5, x6, x7, x8, x9]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    full_txt_input = Input(shape=(full_txt_maxlen,), dtype='int32')\n",
    "    full_txt_mask = Input(shape=(full_txt_maxlen,), dtype='float32')\n",
    "    full_txt_gaussian = Input(shape=(full_txt_maxlen,), dtype='float32')\n",
    "    context_txt_input = Input(shape=(full_txt_maxlen,), dtype='int32')\n",
    "    \n",
    "    numeric_inputs = Input(shape=(5,),name=\"numeric_input\", dtype='float32')\n",
    "    \n",
    "    full_txt_sentiments = Input(shape=(full_txt_maxlen,), dtype='float32')\n",
    "    full_txt_counts = Input(shape=(full_txt_maxlen,), dtype='float32')\n",
    "    \n",
    "    context_txt_sentiments = Input(shape=(full_txt_maxlen,), dtype='float32')\n",
    "    context_txt_counts = Input(shape=(full_txt_maxlen,), dtype='float32')\n",
    "    \n",
    "    \n",
    "    fc_layer_width = conv_embedded_params[\"fc_layer_width\"]\n",
    "    full_text_conv_layer_width = conv_embedded_params[\"full_text_conv_layer_width\"]\n",
    "    context_conv_layer_width = conv_embedded_params[\"context_conv_layer_width\"]\n",
    "    fc_layer_depth = conv_embedded_params[\"fc_layer_depth\"]\n",
    "    lstm_context_units = conv_embedded_params[\"lstm_context_units\"]\n",
    "    lstm_full_text_units = conv_embedded_params[\"lstm_full_text_units\"]\n",
    "    \n",
    "    \n",
    "    embedding_layer = Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=full_txt_maxlen,\n",
    "                    name=\"txt_embedding\",)\n",
    "    \n",
    "    \n",
    "    \n",
    "    full_txt_embedding_out = embedding_layer(full_txt_input)\n",
    "    context_txt_embedding_out = embedding_layer(context_txt_input) # 25 channels\n",
    "    \n",
    "    # https://github.com/keras-team/keras/issues/5474\n",
    "    # print(\"Before Channel select, \",K.int_shape(context_txt_input),K.int_shape(context_txt_embedding_out))\n",
    "    context_txt_input_select = Lambda(lambda x : x[:,full_txt_maxlen-context_txt_maxlen:full_txt_maxlen])(context_txt_input)\n",
    "    context_txt_embedding_out = Lambda(lambda x : x[:,full_txt_maxlen-context_txt_maxlen:full_txt_maxlen,:])(context_txt_embedding_out)\n",
    "    context_txt_sentiments_out = Lambda(lambda x : x[:,full_txt_maxlen-context_txt_maxlen:full_txt_maxlen])(context_txt_sentiments)\n",
    "    context_txt_counts_out = Lambda(lambda x : x[:,full_txt_maxlen-context_txt_maxlen:full_txt_maxlen])(context_txt_counts)\n",
    "    \n",
    "    # print(\"After Channel select, \",K.int_shape(context_txt_input_select),K.int_shape(context_txt_embedding_out))\n",
    "    \n",
    "    # print(\"Pre Masking, \",K.int_shape(full_txt_embedding_out),K.int_shape(context_txt_embedding_out))\n",
    "    \n",
    "    # https://github.com/keras-team/keras/issues/9311\n",
    "    \n",
    "    def mask_zeros(ip):\n",
    "        txt = ip[0]\n",
    "        txt_out = ip[1]\n",
    "        mask = K.cast(K.not_equal(txt, 0), 'float32')\n",
    "        expanded_mask = K.tile(K.expand_dims(mask, axis=2), n=[1, 1, embedding_dims])\n",
    "        out = Multiply()([expanded_mask,  txt_out])\n",
    "        return out\n",
    "    \n",
    "    full_txt_embedding_out = Lambda(mask_zeros)([full_txt_input, full_txt_embedding_out])\n",
    "    context_txt_embedding_out = Lambda(mask_zeros)([context_txt_input_select, context_txt_embedding_out])\n",
    "    \n",
    "    # print(\"Post Masking, \",K.int_shape(full_txt_embedding_out),K.int_shape(context_txt_embedding_out))\n",
    "    \n",
    "    \n",
    "    # https://stackoverflow.com/questions/53849829/element-wise-multiplication-with-keras\n",
    "    # print(K.int_shape(full_txt_gaussian),K.int_shape(full_txt_embedding_out))\n",
    "    \n",
    "    full_txt_sentiments_out = Reshape((full_txt_maxlen,1))(full_txt_sentiments) \n",
    "    full_txt_counts_out = Reshape((full_txt_maxlen,1))(full_txt_counts) \n",
    "    context_txt_sentiments_out = Reshape((context_txt_maxlen,1))(context_txt_sentiments_out) \n",
    "    context_txt_counts_out = Reshape((context_txt_maxlen,1))(context_txt_counts_out) \n",
    "    \n",
    "    \n",
    "    full_txt_embedding_out = concatenate([full_txt_sentiments_out,full_txt_embedding_out,full_txt_counts_out])\n",
    "    context_txt_embedding_out = concatenate([context_txt_sentiments_out,context_txt_embedding_out,context_txt_counts_out])\n",
    "    \n",
    "    \n",
    "    gaussian_mask_out = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([full_txt_embedding_out, full_txt_gaussian])\n",
    "    full_txt_gaussian_out = Reshape((full_txt_maxlen,1))(full_txt_gaussian) \n",
    "    full_txt_mask_out = Reshape((full_txt_maxlen,1))(full_txt_mask) \n",
    "    \n",
    "    fc_input0 = GlobalAveragePooling1D()(context_txt_embedding_out)\n",
    "    fc_input0 = fc_layer(fc_input0,int(fc_layer_width/4),)\n",
    "    \n",
    "    \n",
    "    fc_input1 = GlobalAveragePooling1D()(gaussian_mask_out)\n",
    "    fc_input1 = fc_layer(fc_input1,int(fc_layer_width/4),)\n",
    "    \n",
    "    \n",
    "    full_txt_embedding_out = concatenate([full_txt_mask_out,full_txt_embedding_out,full_txt_gaussian_out]) # 27 channels now\n",
    "    full_txt_embedding_out = Dropout(0.3)(full_txt_embedding_out)\n",
    "    context_txt_embedding_out = Dropout(0.3)(context_txt_embedding_out)\n",
    "    \n",
    "    \n",
    "    \n",
    "    fc_input2 = conv_pipe(context_txt_embedding_out, context_conv_layer_width)\n",
    "    fc_input3 = conv_pipe(full_txt_embedding_out, full_text_conv_layer_width)\n",
    "    \n",
    "#     fc_input4 = Bidirectional(GRU(lstm_context_units,dropout=0.1, return_sequences=True, implementation=2, unroll=True))(context_txt_embedding_out)\n",
    "#     fc_input5 = Bidirectional(GRU(lstm_full_text_units, dropout=0.1, return_sequences=True, implementation=2, unroll=True))(full_txt_embedding_out)\n",
    "#     fc_input4 = Bidirectional(GRU(int(lstm_context_units/2), implementation=2, unroll=True))(fc_input4)\n",
    "#     fc_input5 = Bidirectional(GRU(int(lstm_full_text_units/2), implementation=2, unroll=True))(fc_input5)\n",
    "    \n",
    "    \n",
    "    numerics = fc_layer(numeric_inputs,12,)\n",
    "    numerics = fc_layer(numerics,8,)\n",
    "    numerics = Dropout(0.2)(numerics)\n",
    "    \n",
    "    fc_input = concatenate([numerics,fc_input0,fc_input1,fc_input2,fc_input3])\n",
    "    fc_input = Dropout(0.1)(fc_input)\n",
    "    print(\"FC Input dims = \", K.int_shape(fc_input))\n",
    "    \n",
    "    for i in range(fc_layer_depth-1):\n",
    "        fc_input = fc_layer(fc_input,fc_layer_width)\n",
    "        fc_input = Dropout(0.1)(fc_input)\n",
    "    \n",
    "    fc_out = fc_layer(fc_input,int(fc_layer_width/2),bn=False)\n",
    "    fc_out = Dense(3)(fc_out)\n",
    "    out = Activation(\"softmax\")(fc_out)\n",
    "    # vs sigmoid\n",
    "    \n",
    "    model = Model(inputs=[full_txt_input,full_txt_mask,full_txt_gaussian,context_txt_input,numeric_inputs,\n",
    "                         full_txt_sentiments,full_txt_counts,context_txt_sentiments,context_txt_counts], \n",
    "                  outputs=[out])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n"
     ]
    }
   ],
   "source": [
    "reset_keras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'full_text_conv_layer_width': 8,\n",
       " 'context_conv_layer_width': 6,\n",
       " 'lstm_full_text_units': 16,\n",
       " 'lstm_context_units': 8,\n",
       " 'fc_layer_width': 96,\n",
       " 'fc_layer_depth': 2,\n",
       " 'training_policy': {'epochs': 6,\n",
       "  'batch_size': 64,\n",
       "  'policy': 'adam',\n",
       "  'lr': 0.003,\n",
       "  'max_lr_olr': 0.1,\n",
       "  'lr_shed_fn': <function params.lr_decay(epoch, prev_lr)>}}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FC Input dims =  (None, 552)\n",
      "Before Resampling, Size =  (3695, 22) \n",
      " 2    2654\n",
      "1     598\n",
      "0     443\n",
      "Name: sentiment, dtype: int64\n",
      "After Resampling =  (9786, 22) \n",
      " 1    3588\n",
      "0    3544\n",
      "2    2654\n",
      "Name: sentiment, dtype: int64\n",
      "Total Model Params =  1092551\n",
      "Epoch 1/6\n",
      "Epoch = 1, Prev LR = 0.0030, New LR = 0.0002\n",
      " - 46s - loss: 1.2611 - acc: 0.3433 - val_loss: 1.2365 - val_acc: 0.2364\n",
      " - 31s - loss: 1.1014 - acc: 0.4218 - val_loss: 1.3610 - val_acc: 0.2399\n",
      "train-test f1 correlation = \n",
      "           f1_train   f1_test\n",
      "f1_train  1.000000  0.991046\n",
      "f1_test   0.991046  1.000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p1_by_p0</th>\n",
       "      <th>p2_by_p0</th>\n",
       "      <th>acc_train</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>acc_test</th>\n",
       "      <th>f1_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2550</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.249374</td>\n",
       "      <td>0.247189</td>\n",
       "      <td>0.234965</td>\n",
       "      <td>0.233941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>0.392745</td>\n",
       "      <td>2.082666</td>\n",
       "      <td>0.663053</td>\n",
       "      <td>0.432506</td>\n",
       "      <td>0.680420</td>\n",
       "      <td>0.425400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>0.616994</td>\n",
       "      <td>2.082666</td>\n",
       "      <td>0.640534</td>\n",
       "      <td>0.452152</td>\n",
       "      <td>0.638462</td>\n",
       "      <td>0.424245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>0.572250</td>\n",
       "      <td>2.082666</td>\n",
       "      <td>0.650820</td>\n",
       "      <td>0.456143</td>\n",
       "      <td>0.648252</td>\n",
       "      <td>0.423194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>0.456563</td>\n",
       "      <td>2.082666</td>\n",
       "      <td>0.658882</td>\n",
       "      <td>0.437096</td>\n",
       "      <td>0.672028</td>\n",
       "      <td>0.422949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      p1_by_p0  p2_by_p0  acc_train  f1_train  acc_test   f1_test\n",
       "2550  1.000000  1.000000   0.249374  0.247189  0.234965  0.233941\n",
       "318   0.392745  2.082666   0.663053  0.432506  0.680420  0.425400\n",
       "624   0.616994  2.082666   0.640534  0.452152  0.638462  0.424245\n",
       "573   0.572250  2.082666   0.650820  0.456143  0.648252  0.423194\n",
       "420   0.456563  2.082666   0.658882  0.437096  0.672028  0.422949"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/6\n",
      "Epoch = 3, Prev LR = 0.0006, New LR = 0.0030\n",
      " - 30s - loss: 0.9263 - acc: 0.5542 - val_loss: 1.4945 - val_acc: 0.3038\n",
      "train-test f1 correlation = \n",
      "           f1_train   f1_test\n",
      "f1_train  1.000000  0.979825\n",
      "f1_test   0.979825  1.000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p1_by_p0</th>\n",
       "      <th>p2_by_p0</th>\n",
       "      <th>acc_train</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>acc_test</th>\n",
       "      <th>f1_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2550</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.338154</td>\n",
       "      <td>0.370248</td>\n",
       "      <td>0.304589</td>\n",
       "      <td>0.316215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.269547</td>\n",
       "      <td>2.501926</td>\n",
       "      <td>0.716908</td>\n",
       "      <td>0.639738</td>\n",
       "      <td>0.640823</td>\n",
       "      <td>0.511708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.290623</td>\n",
       "      <td>2.501926</td>\n",
       "      <td>0.709121</td>\n",
       "      <td>0.635607</td>\n",
       "      <td>0.634494</td>\n",
       "      <td>0.511167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.269547</td>\n",
       "      <td>2.353547</td>\n",
       "      <td>0.709399</td>\n",
       "      <td>0.635917</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>0.509656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>2.501926</td>\n",
       "      <td>0.724694</td>\n",
       "      <td>0.644165</td>\n",
       "      <td>0.646361</td>\n",
       "      <td>0.509235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      p1_by_p0  p2_by_p0  acc_train  f1_train  acc_test   f1_test\n",
       "2550  1.000000  1.000000   0.338154  0.370248  0.304589  0.316215\n",
       "66    0.269547  2.501926   0.716908  0.639738  0.640823  0.511708\n",
       "117   0.290623  2.501926   0.709121  0.635607  0.634494  0.511167\n",
       "65    0.269547  2.353547   0.709399  0.635917  0.632911  0.509656\n",
       "15    0.250000  2.501926   0.724694  0.644165  0.646361  0.509235"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/6\n",
      "Epoch = 4, Prev LR = 0.0030, New LR = 0.0027\n",
      " - 30s - loss: 0.6825 - acc: 0.7020 - val_loss: 1.9331 - val_acc: 0.2595\n",
      "train-test f1 correlation = \n",
      "           f1_train   f1_test\n",
      "f1_train  1.000000  0.966529\n",
      "f1_test   0.966529  1.000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p1_by_p0</th>\n",
       "      <th>p2_by_p0</th>\n",
       "      <th>acc_train</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>acc_test</th>\n",
       "      <th>f1_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2550</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.269263</td>\n",
       "      <td>0.344491</td>\n",
       "      <td>0.242089</td>\n",
       "      <td>0.243412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.25</td>\n",
       "      <td>4.337497</td>\n",
       "      <td>0.769958</td>\n",
       "      <td>0.629526</td>\n",
       "      <td>0.700949</td>\n",
       "      <td>0.457359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.25</td>\n",
       "      <td>3.396508</td>\n",
       "      <td>0.756606</td>\n",
       "      <td>0.635682</td>\n",
       "      <td>0.667722</td>\n",
       "      <td>0.457221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.25</td>\n",
       "      <td>3.195075</td>\n",
       "      <td>0.752156</td>\n",
       "      <td>0.637481</td>\n",
       "      <td>0.662184</td>\n",
       "      <td>0.455517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.25</td>\n",
       "      <td>7.519758</td>\n",
       "      <td>0.766898</td>\n",
       "      <td>0.565732</td>\n",
       "      <td>0.737342</td>\n",
       "      <td>0.454644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      p1_by_p0  p2_by_p0  acc_train  f1_train  acc_test   f1_test\n",
       "2550      1.00  1.000000   0.269263  0.344491  0.242089  0.243412\n",
       "24        0.25  4.337497   0.769958  0.629526  0.700949  0.457359\n",
       "20        0.25  3.396508   0.756606  0.635682  0.667722  0.457221\n",
       "19        0.25  3.195075   0.752156  0.637481  0.662184  0.455517\n",
       "33        0.25  7.519758   0.766898  0.565732  0.737342  0.454644"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/6\n",
      "Epoch = 5, Prev LR = 0.0027, New LR = 0.0024\n",
      " - 30s - loss: 0.4902 - acc: 0.7994 - val_loss: 1.2255 - val_acc: 0.4755\n",
      "train-test f1 correlation = \n",
      "           f1_train   f1_test\n",
      "f1_train  1.000000  0.951842\n",
      "f1_test   0.951842  1.000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p1_by_p0</th>\n",
       "      <th>p2_by_p0</th>\n",
       "      <th>acc_train</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>acc_test</th>\n",
       "      <th>f1_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2550</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.619471</td>\n",
       "      <td>0.623542</td>\n",
       "      <td>0.492880</td>\n",
       "      <td>0.454675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1.733663</td>\n",
       "      <td>0.832823</td>\n",
       "      <td>0.743073</td>\n",
       "      <td>0.720728</td>\n",
       "      <td>0.537213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1.842962</td>\n",
       "      <td>0.833102</td>\n",
       "      <td>0.741328</td>\n",
       "      <td>0.725475</td>\n",
       "      <td>0.536389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1.130065</td>\n",
       "      <td>0.826704</td>\n",
       "      <td>0.757159</td>\n",
       "      <td>0.675633</td>\n",
       "      <td>0.535501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.25</td>\n",
       "      <td>2.082666</td>\n",
       "      <td>0.830876</td>\n",
       "      <td>0.732722</td>\n",
       "      <td>0.734177</td>\n",
       "      <td>0.535329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      p1_by_p0  p2_by_p0  acc_train  f1_train  acc_test   f1_test\n",
       "2550      1.00  1.000000   0.619471  0.623542  0.492880  0.454675\n",
       "9         0.25  1.733663   0.832823  0.743073  0.720728  0.537213\n",
       "10        0.25  1.842962   0.833102  0.741328  0.725475  0.536389\n",
       "2         0.25  1.130065   0.826704  0.757159  0.675633  0.535501\n",
       "12        0.25  2.082666   0.830876  0.732722  0.734177  0.535329"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/6\n",
      "Epoch = 6, Prev LR = 0.0024, New LR = 0.0022\n",
      " - 30s - loss: 0.3950 - acc: 0.8463 - val_loss: 1.1845 - val_acc: 0.5451\n",
      "train-test f1 correlation = \n",
      "           f1_train   f1_test\n",
      "f1_train  1.000000  0.973384\n",
      "f1_test   0.973384  1.000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p1_by_p0</th>\n",
       "      <th>p2_by_p0</th>\n",
       "      <th>acc_train</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>acc_test</th>\n",
       "      <th>f1_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2550</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.722736</td>\n",
       "      <td>0.730502</td>\n",
       "      <td>0.544304</td>\n",
       "      <td>0.465280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>2.827338</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.858828</td>\n",
       "      <td>0.723892</td>\n",
       "      <td>0.524632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>2.082666</td>\n",
       "      <td>0.897393</td>\n",
       "      <td>0.859721</td>\n",
       "      <td>0.702532</td>\n",
       "      <td>0.524576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>2.659660</td>\n",
       "      <td>0.903000</td>\n",
       "      <td>0.861232</td>\n",
       "      <td>0.722310</td>\n",
       "      <td>0.524155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.269547</td>\n",
       "      <td>2.082666</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>0.859416</td>\n",
       "      <td>0.699367</td>\n",
       "      <td>0.523918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      p1_by_p0  p2_by_p0  acc_train  f1_train  acc_test   f1_test\n",
       "2550  1.000000  1.000000   0.722736  0.730502  0.544304  0.465280\n",
       "17    0.250000  2.827338   0.902439  0.858828  0.723892  0.524632\n",
       "12    0.250000  2.082666   0.897393  0.859721  0.702532  0.524576\n",
       "16    0.250000  2.659660   0.903000  0.861232  0.722310  0.524155\n",
       "63    0.269547  2.082666   0.896552  0.859416  0.699367  0.523918"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.74, Macro F1 = 0.75\n",
      "Accuracy = 0.53, Macro F1 = 0.44\n",
      "train-test f1 correlation = \n",
      "           f1_train   f1_test\n",
      "f1_train  1.000000  0.895394\n",
      "f1_test   0.895394  1.000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p1_by_p0</th>\n",
       "      <th>p2_by_p0</th>\n",
       "      <th>acc_train</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>acc_test</th>\n",
       "      <th>f1_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2550</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.742896</td>\n",
       "      <td>0.752897</td>\n",
       "      <td>0.530303</td>\n",
       "      <td>0.442341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.277046</td>\n",
       "      <td>0.887957</td>\n",
       "      <td>0.866091</td>\n",
       "      <td>0.667929</td>\n",
       "      <td>0.494212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.269547</td>\n",
       "      <td>1.277046</td>\n",
       "      <td>0.885250</td>\n",
       "      <td>0.863768</td>\n",
       "      <td>0.664141</td>\n",
       "      <td>0.494145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.313347</td>\n",
       "      <td>1.277046</td>\n",
       "      <td>0.869824</td>\n",
       "      <td>0.850049</td>\n",
       "      <td>0.654040</td>\n",
       "      <td>0.492254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.201309</td>\n",
       "      <td>0.884709</td>\n",
       "      <td>0.862717</td>\n",
       "      <td>0.662247</td>\n",
       "      <td>0.491990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      p1_by_p0  p2_by_p0  acc_train  f1_train  acc_test   f1_test\n",
       "2550  1.000000  1.000000   0.742896  0.752897  0.530303  0.442341\n",
       "4     0.250000  1.277046   0.887957  0.866091  0.667929  0.494212\n",
       "55    0.269547  1.277046   0.885250  0.863768  0.664141  0.494145\n",
       "157   0.313347  1.277046   0.869824  0.850049  0.654040  0.492254\n",
       "3     0.250000  1.201309   0.884709  0.862717  0.662247  0.491990"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.28      0.29       174\n",
      "           1       0.27      0.77      0.40       239\n",
      "           2       0.83      0.52      0.64      1171\n",
      "\n",
      "   micro avg       0.53      0.53      0.53      1584\n",
      "   macro avg       0.47      0.52      0.44      1584\n",
      "weighted avg       0.68      0.53      0.56      1584\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "conv_embedded_params\n",
    "looped_random_validation(df,build_model,train_generator_model_with_validation,extract_values,\n",
    "                        **conv_embedded_params[\"training_policy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Accuracy = 0.89, Macro F1 = 0.86\n",
    "Accuracy = 0.67, Macro F1 = 0.55\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.34      0.47      0.39       184\n",
    "           1       0.42      0.55      0.47       249\n",
    "           2       0.84      0.73      0.78      1151\n",
    "\n",
    "   micro avg       0.67      0.67      0.67      1584\n",
    "   macro avg       0.53      0.58      0.55      1584\n",
    "weighted avg       0.71      0.67      0.69      1584\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "Accuracy = 0.92, Macro F1 = 0.89\n",
    "Accuracy = 0.67, Macro F1 = 0.53\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.31      0.37      0.34       199\n",
    "           1       0.44      0.52      0.48       248\n",
    "           2       0.81      0.76      0.78      1137\n",
    "\n",
    "   micro avg       0.67      0.67      0.67      1584\n",
    "   macro avg       0.52      0.55      0.53      1584\n",
    "weighted avg       0.69      0.67      0.68      1584\n",
    "```\n",
    "\n",
    "```\n",
    "Accuracy = 0.92, Macro F1 = 0.90\n",
    "Accuracy = 0.66, Macro F1 = 0.39\n",
    "Accuracy = 0.67, Macro F1 = 0.51\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x7f97a34c6ba8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1455, in __del__\n",
      "    self._session._session, self._handle, status)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 528, in __exit__\n",
      "    c_api.TF_GetCode(self.status.status))\n",
      "tensorflow.python.framework.errors_impl.CancelledError: Session has been closed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "901071\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FC Input dims =  (None, 552)\n",
      "Before Resampling, Size =  (5279, 22) \n",
      " 2    3825\n",
      "1     837\n",
      "0     617\n",
      "Name: sentiment, dtype: int64\n",
      "After Resampling =  (13783, 22) \n",
      " 1    5022\n",
      "0    4936\n",
      "2    3825\n",
      "Name: sentiment, dtype: int64\n",
      "Total Model Params =  1092551\n",
      "Epoch 1/6\n",
      "Epoch = 1, Prev LR = 0.0030, New LR = 0.0002\n",
      "216/216 [==============================] - 58s 267ms/step - loss: 1.2362 - acc: 0.3508\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p1_by_p0</th>\n",
       "      <th>p2_by_p0</th>\n",
       "      <th>f1_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.192608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.192608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.9</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.419780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.9</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.414661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0.408831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.408312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     p1_by_p0  p2_by_p0  f1_train\n",
       "887       1.0      1.00  0.192608\n",
       "851       1.0      1.00  0.192608\n",
       "78        0.9      2.00  0.419780\n",
       "77        0.9      1.75  0.414661\n",
       "856       1.0      2.25  0.408831\n",
       "855       1.0      2.00  0.408312"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/6\n",
      "Epoch = 2, Prev LR = 0.0002, New LR = 0.0006\n",
      "216/216 [==============================] - 45s 209ms/step - loss: 1.0858 - acc: 0.4295\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p1_by_p0</th>\n",
       "      <th>p2_by_p0</th>\n",
       "      <th>f1_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.244593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.244593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.9</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0.485028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0.479157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.9</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.477154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.7</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.473358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     p1_by_p0  p2_by_p0  f1_train\n",
       "887       1.0      1.00  0.244593\n",
       "851       1.0      1.00  0.244593\n",
       "79        0.9      2.25  0.485028\n",
       "856       1.0      2.25  0.479157\n",
       "78        0.9      2.00  0.477154\n",
       "41        0.7      2.00  0.473358"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/6\n",
      "Epoch = 3, Prev LR = 0.0006, New LR = 0.0030\n",
      "216/216 [==============================] - 45s 210ms/step - loss: 0.8723 - acc: 0.5962\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p1_by_p0</th>\n",
       "      <th>p2_by_p0</th>\n",
       "      <th>f1_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.516234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.516234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.630129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.630009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0.629359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.627490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     p1_by_p0  p2_by_p0  f1_train\n",
       "851       1.0      1.00  0.516234\n",
       "887       1.0      1.00  0.516234\n",
       "6         0.5      2.50  0.630129\n",
       "7         0.5      2.75  0.630009\n",
       "5         0.5      2.25  0.629359\n",
       "4         0.5      2.00  0.627490"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/6\n",
      "Epoch = 4, Prev LR = 0.0030, New LR = 0.0027\n",
      "216/216 [==============================] - 43s 197ms/step - loss: 0.6133 - acc: 0.7426\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p1_by_p0</th>\n",
       "      <th>p2_by_p0</th>\n",
       "      <th>f1_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.485728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.485728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.7</td>\n",
       "      <td>8.50</td>\n",
       "      <td>0.755897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.7</td>\n",
       "      <td>8.25</td>\n",
       "      <td>0.754202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.7</td>\n",
       "      <td>8.75</td>\n",
       "      <td>0.753862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.5</td>\n",
       "      <td>7.50</td>\n",
       "      <td>0.752886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     p1_by_p0  p2_by_p0  f1_train\n",
       "851       1.0      1.00  0.485728\n",
       "887       1.0      1.00  0.485728\n",
       "67        0.7      8.50  0.755897\n",
       "66        0.7      8.25  0.754202\n",
       "68        0.7      8.75  0.753862\n",
       "26        0.5      7.50  0.752886"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/6\n",
      "Epoch = 5, Prev LR = 0.0027, New LR = 0.0024\n",
      "216/216 [==============================] - 42s 197ms/step - loss: 0.4676 - acc: 0.8130\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p1_by_p0</th>\n",
       "      <th>p2_by_p0</th>\n",
       "      <th>f1_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.635535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.635535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.5</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.853032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.5</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.852849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.5</td>\n",
       "      <td>5.75</td>\n",
       "      <td>0.852370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.5</td>\n",
       "      <td>4.75</td>\n",
       "      <td>0.851841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     p1_by_p0  p2_by_p0  f1_train\n",
       "851       1.0      1.00  0.635535\n",
       "887       1.0      1.00  0.635535\n",
       "16        0.5      5.00  0.853032\n",
       "20        0.5      6.00  0.852849\n",
       "19        0.5      5.75  0.852370\n",
       "15        0.5      4.75  0.851841"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/6\n",
      "Epoch = 6, Prev LR = 0.0024, New LR = 0.0022\n",
      "216/216 [==============================] - 43s 197ms/step - loss: 0.3988 - acc: 0.8466\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p1_by_p0</th>\n",
       "      <th>p2_by_p0</th>\n",
       "      <th>f1_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.764920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.764920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.9</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0.868378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.867891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0.867645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.25</td>\n",
       "      <td>0.867609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     p1_by_p0  p2_by_p0  f1_train\n",
       "851       1.0      1.00  0.764920\n",
       "887       1.0      1.00  0.764920\n",
       "85        0.9      3.75  0.868378\n",
       "863       1.0      4.00  0.867891\n",
       "862       1.0      3.75  0.867645\n",
       "864       1.0      4.25  0.867609"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reset_keras()\n",
    "gc.collect()\n",
    "model = build_model()\n",
    "metrics = train_generator_model_with_validation(model,df,None,extract_values,[1,1,1],\n",
    "                   **conv_embedded_params[\"training_policy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Predictions - No cutout\n"
     ]
    }
   ],
   "source": [
    "y_test_preds = model.predict(extract_values(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y_test_preds_out = prediction_caliberation_wrapper(y_test_preds,[9,2,1])\n",
    "y_test_preds_out = np.argmax(y_test_preds_out, axis=1)\n",
    "assert len(y_test_preds_out)==df_test.shape[0]\n",
    "df_test['sentiment'] = y_test_preds_out\n",
    "df_test[['unique_hash','sentiment']].to_csv(\"submission.csv\",index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# M2: CNN Model - No Padding and Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5279, 16)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2924, 14)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_np = read_csv(\"train_non_padded.csv\")\n",
    "df_test_np = read_csv(\"test_non_padded.csv\")\n",
    "\n",
    "df_np.shape\n",
    "df_test_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410\n"
     ]
    }
   ],
   "source": [
    "reset_keras()\n",
    "trained_embeddings = PreTrainedEmbeddingsTransformer(fb_model,size=fasttext_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_values_pretrained(x):\n",
    "    x1 = x[\"full_txt\"].values\n",
    "    x2 = x[\"full_txt_mask\"].values\n",
    "    x3 = x[\"full_txt_mask_gaussian\"].values\n",
    "    x4 = x[\"context_txt\"].values\n",
    "    x5 = x[[\"occurences_count\",\"review_length\",\"review_length_by_occurences_count\"]].values\n",
    "    train_phase = False\n",
    "    if 'ohe_labels' in x.columns:\n",
    "        train_phase = True\n",
    "        y = np.stack(x['ohe_labels'].values,axis=0)\n",
    "\n",
    "    \n",
    "\n",
    "    full_txt_maxlen = 0\n",
    "    for x in x1:\n",
    "        full_txt_maxlen = max(full_txt_maxlen,len(x))\n",
    "        \n",
    "    context_txt_maxlen = 0\n",
    "    for x in x4:\n",
    "        context_txt_maxlen = max(context_txt_maxlen,len(x))\n",
    "    \n",
    "    x1 = list(pad_text_sequences(x1, maxlen=full_txt_maxlen,jobs=1))\n",
    "    x2 = list(pad_text_sequences(x2, maxlen=full_txt_maxlen,empty=0.0,jobs=1))\n",
    "    x3 = list(pad_text_sequences(x3, maxlen=full_txt_maxlen,empty=0.0,jobs=1))\n",
    "    x4 = list(pad_text_sequences(x4, maxlen=context_txt_maxlen,jobs=1))\n",
    "    \n",
    "    x1 = np.array(x1)\n",
    "    x4 = np.array(x4)\n",
    "    \n",
    "    x1 = np.stack(x1, axis=0)\n",
    "    x4 = np.stack(x4, axis=0)\n",
    "    \n",
    "    x2 = np.asarray([np.asarray(x) for x in x2])\n",
    "    x3 = np.asarray([np.asarray(x) for x in x3])\n",
    "    \n",
    "#     print(x1.shape,x2.shape,x3.shape,x4.shape)\n",
    "    if train_phase:\n",
    "        x1,x2,x3 = batch_cutout( x1,x2,x3,p=cutout_proba, min_words=min_cutout, max_words=max_cutout,)\n",
    "        \n",
    "        x4 = batch_cutout(x4, p=cutout_proba, min_words=min_cutout, max_words=max_cutout,)[0]\n",
    "    \n",
    "    \n",
    "    x4 = trained_embeddings.transform(x4)\n",
    "    x1 = trained_embeddings.transform(x1)\n",
    "    \n",
    "    # Batch padding!!\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     x2 = np.asarray([np.asarray(x).reshape((len(x),1)) for x in x2])\n",
    "#     x3 = np.asarray([np.asarray(x).reshape((len(x),1)) for x in x3])\n",
    "#     x2 = np.stack(x2, axis=0)\n",
    "#     x3 = np.stack(x3, axis=0)\n",
    "    x2 = x2.reshape((x2.shape[0],x2.shape[1],1))\n",
    "    x3 = x3.reshape((x3.shape[0],x3.shape[1],1))\n",
    "#     print(x1.shape,x2.shape,x3.shape,x4.shape)\n",
    "    if train_phase:\n",
    "        return [x1,x2,x3,x4,x5],y\n",
    "    else:\n",
    "        return [x1,x2,x3,x4,x5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 7204) (1000, 7204) (1000, 7204) (1000, 1267)\n",
      "(1000, 7204, 300) (1000, 7204, 1) (1000, 7204, 1) (1000, 1267, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1656"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = df_np.sample(1000)\n",
    "_ = extract_values_pretrained(sample)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_model_no_embedding():\n",
    "    full_txt_input = Input(shape=(None,fasttext_dims), dtype='float32')\n",
    "    full_txt_mask = Input(shape=(None,1), dtype='float32')\n",
    "    full_txt_gaussian = Input(shape=(None,1), dtype='float32')\n",
    "    \n",
    "    context_txt_input = Input(shape=(None,fasttext_dims), dtype='float32')\n",
    "    \n",
    "    numeric_inputs = Input(shape=(3,),name=\"numeric_input\", dtype='float32')\n",
    "    \n",
    "    fc_layer_width = conv_non_embedded_params[\"fc_layer_width\"]\n",
    "    full_text_conv_layer_width = conv_non_embedded_params[\"full_text_conv_layer_width\"]\n",
    "    context_conv_layer_width = conv_non_embedded_params[\"context_conv_layer_width\"]\n",
    "    fc_layer_depth = conv_non_embedded_params[\"fc_layer_depth\"]\n",
    "    \n",
    "    \n",
    "    full_txt_embedding_out = full_txt_input\n",
    "    context_txt_embedding_out = context_txt_input # 25 channels\n",
    "    \n",
    "    full_txt_embedding_out = Dropout(0.1)(full_txt_embedding_out)\n",
    "    context_txt_embedding_out = Dropout(0.1)(context_txt_embedding_out)\n",
    "    # print(K.int_shape(full_txt_embedding_out))\n",
    "    \n",
    "    # https://stackoverflow.com/questions/53849829/element-wise-multiplication-with-keras\n",
    "    gaussian_mask_out = Lambda(lambda x: x[0] * x[1])([full_txt_embedding_out, full_txt_gaussian])\n",
    "    # print(K.int_shape(gaussian_mask_out))\n",
    "    full_txt_gaussian_out = full_txt_gaussian\n",
    "    full_txt_mask_out = full_txt_mask\n",
    "    # print(K.int_shape(full_txt_mask_out))\n",
    "    \n",
    "    fc_input1 = GlobalAveragePooling1D()(gaussian_mask_out)\n",
    "    fc_input1 = fc_layer(fc_input1,fc_layer_width,)\n",
    "    \n",
    "    full_txt_embedding_masked = concatenate([full_txt_mask_out,full_txt_embedding_out,full_txt_gaussian_out]) # 27 channels now\n",
    "    # print(K.int_shape(full_txt_embedding_masked))\n",
    "    \n",
    "    fc_input2 = conv_pipe(context_txt_embedding_out, context_conv_layer_width)\n",
    "    fc_input3 = conv_pipe(full_txt_embedding_masked, full_text_conv_layer_width)\n",
    "    \n",
    "    \n",
    "    numerics = fc_layer(numeric_inputs,8,)\n",
    "    numerics = fc_layer(numerics,8,)\n",
    "    \n",
    "    fc_input = concatenate([numerics,fc_input1,fc_input2,fc_input3])\n",
    "    fc_input = Dropout(0.1)(fc_input)\n",
    "    \n",
    "    for i in range(fc_layer_depth-1):\n",
    "        fc_input = fc_layer(fc_input,fc_layer_width)\n",
    "    \n",
    "    fc_out = fc_layer(fc_input,int(fc_layer_width/2),bn=False)\n",
    "    fc_out = Dense(3)(fc_out)\n",
    "    out = Activation(\"softmax\")(fc_out)\n",
    "    \n",
    "    model = Model(inputs=[full_txt_input,full_txt_mask,full_txt_gaussian,context_txt_input,numeric_inputs], \n",
    "                  outputs=[out])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'full_text_conv_layer_width': 6, 'context_conv_layer_width': 4, 'fc_layer_width': 80, 'fc_layer_depth': 2, 'training_policy': {'epochs': 10, 'batch_size': 32, 'policy': 'adam', 'lr': 0.005, 'max_lr_olr': 0.002, 'lr_shed_fn': <function <lambda> at 0x7fb5e24157b8>}}\n",
      "Before Resampling, Size =  (3695, 16) \n",
      " 2    2696\n",
      "1     578\n",
      "0     421\n",
      "Name: sentiment, dtype: int64\n",
      "After Resampling =  (8533, 16) \n",
      " 0    2947\n",
      "1    2890\n",
      "2    2696\n",
      "Name: sentiment, dtype: int64\n",
      "Epoch 1/10\n",
      " 70/267 [======>.......................] - ETA: 5:22 - loss: 1.2108 - acc: 0.3379"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[32,13735,96] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node training/Adam/gradients/zeros_73}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node metrics/acc/Mean}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-359-7cdb860b49ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_non_embedded_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m looped_random_validation(df_np,build_model_no_embedding,train_generator_model_with_validation,extract_values_pretrained,\n\u001b[0;32m----> 4\u001b[0;31m                         **conv_non_embedded_params[\"training_policy\"])\n\u001b[0m",
      "\u001b[0;32m<ipython-input-229-ca35b1ca5e5b>\u001b[0m in \u001b[0;36mlooped_random_validation\u001b[0;34m(df, model_fn, training_fn, extract_values, weights, epochs, batch_size, policy, lr, max_lr_olr, lr_shed_fn)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     training_fn(model,df_train,df_val,extract_values,weights,\n\u001b[0;32m---> 14\u001b[0;31m                epochs=epochs, batch_size=batch_size, policy=policy,lr=lr,max_lr_olr=max_lr_olr, lr_shed_fn=lr_shed_fn)\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0my_train_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_test_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-285-c7d8f39bdc08>\u001b[0m in \u001b[0;36mtrain_generator_model_with_validation\u001b[0;34m(model, train, val, extract_values, weights, epochs, batch_size, policy, lr, max_lr_olr, lr_shed_fn)\u001b[0m\n\u001b[1;32m     52\u001b[0m                         \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                         callbacks=callbacks,)\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[32,13735,96] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node training/Adam/gradients/zeros_73}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node metrics/acc/Mean}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "print(conv_non_embedded_params)\n",
    "looped_random_validation(df_np,build_model_no_embedding,train_generator_model_with_validation,extract_values_pretrained,\n",
    "                        **conv_non_embedded_params[\"training_policy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = build_model_no_embedding()\n",
    "train_generator_model_with_validation(model,df_np,None,extract_values_pretrained,\n",
    "                   **conv_non_embedded_params[\"training_policy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "iterator = MakeIter(df_test_np,extract_values,batch_size)\n",
    "y_test_preds = model.predict_generator(iterator,steps=len(iterator))\n",
    "y_test_preds = np.argmax(y_test_preds, axis=1)\n",
    "df_test_np['sentiment'] = y_test_preds\n",
    "df_test_np[['unique_hash','sentiment']].to_csv(\"submission.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "320px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
