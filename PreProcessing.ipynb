{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "- stopword\n",
    "- hyphen\n",
    "\n",
    "(Target) 0-positive, 1-negative, 2-neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T16:20:07.179065Z",
     "start_time": "2019-07-24T16:20:01.945144Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Reshape, Multiply\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "%matplotlib inline\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, DepthwiseConv2D, Conv2D, SeparableConv2D, MaxPooling1D, AveragePooling1D\n",
    "from keras.layers import Input, concatenate, LeakyReLU\n",
    "import gensim.downloader as api\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Activation, Flatten, Dense, Dropout, SpatialDropout1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD, Nadam, Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "from keras.regularizers import l2\n",
    "%config InlineBackend.figure_format='retina'\n",
    "from keras_contrib.callbacks import CyclicLR\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from data_science_utils.vision.keras import *\n",
    "from time import time\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import missingno as msno\n",
    "import re\n",
    "from joblib import Parallel, delayed\n",
    "from data_science_utils import dataframe as df_utils\n",
    "from data_science_utils import models as model_utils\n",
    "from data_science_utils import plots as plot_utils\n",
    "from data_science_utils.dataframe import column as column_utils\n",
    "from data_science_utils import misc as misc\n",
    "from data_science_utils import preprocessing as pp_utils\n",
    "from data_science_utils import nlp as nlp_utils\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from data_science_utils.dataframe import get_specific_cols\n",
    "from random import sample\n",
    "\n",
    "import more_itertools\n",
    "from more_itertools import flatten\n",
    "import ast\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import gc\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd())\n",
    "from importlib import reload\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "reload(model_utils)\n",
    "from data_science_utils.models import mean_absolute_percentage_error\n",
    "from data_science_utils.models import median_absolute_percentage_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "from more_itertools import flatten\n",
    "import dill\n",
    "from collections import Counter\n",
    "import operator\n",
    "from gensim.models import FastText\n",
    "import itertools\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.utils import tokenize\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "\n",
    "import lib\n",
    "reload(lib)\n",
    "from lib import *\n",
    "\n",
    "import params\n",
    "reload(params)\n",
    "from params import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_utils.stopwords_list = {}\n",
    "nlp_utils.is_stopword('an')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T17:35:47.727614Z",
     "start_time": "2019-07-24T17:35:47.363819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5279, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2924, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "df.shape\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Medical Vocab is very different from wiki/normal vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T17:35:50.442598Z",
     "start_time": "2019-07-24T17:35:49.960782Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df.text = df.text.apply(preprocess_string)\n",
    "df_test.text = df_test.text.apply(preprocess_string)\n",
    "\n",
    "df.drug = df.drug.apply(preprocess_string)\n",
    "df_test.drug = df_test.drug.apply(preprocess_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T17:35:52.028763Z",
     "start_time": "2019-07-24T17:35:51.943973Z"
    }
   },
   "outputs": [],
   "source": [
    "df = find_occurence_count(df)\n",
    "df_test = find_occurence_count(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will later remove the sentences where drug name doesn't occur from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T17:35:54.027641Z",
     "start_time": "2019-07-24T17:35:53.997238Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_hash</th>\n",
       "      <th>text</th>\n",
       "      <th>drug</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>occurences</th>\n",
       "      <th>occurences_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>50b51148821b2d0eb23380ca8417b1d9fefd37af</td>\n",
       "      <td>had anyone been returned to a previous chemo ...</td>\n",
       "      <td>nivolumab</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1856</th>\n",
       "      <td>ddb40991ffb9cddac847c77855977610bf4d03ae</td>\n",
       "      <td>any ideas anyone   i m not due to see the doc...</td>\n",
       "      <td>opdivo</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>ae868203b1cebf2c75722bc38c1f7611ad706ab0</td>\n",
       "      <td>be well</td>\n",
       "      <td>tarceva</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2301</th>\n",
       "      <td>6df111aa3032069076b12f81bd87a9d72dc795ad</td>\n",
       "      <td>sandy</td>\n",
       "      <td>opdivo</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2894</th>\n",
       "      <td>f275e928df77f875dc61f97c791b40519effc58c</td>\n",
       "      <td>i hope and pray you guys have the success i h...</td>\n",
       "      <td>opdivo</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3446</th>\n",
       "      <td>f57ddcc186c8557ad4201bb252244bba749f23cf</td>\n",
       "      <td>any advice</td>\n",
       "      <td>opdivo</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3525</th>\n",
       "      <td>65fb06982b3518557c8244b947fb36ae33b15534</td>\n",
       "      <td>chuck</td>\n",
       "      <td>opdivo</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3720</th>\n",
       "      <td>c40d519b9b37c577ff6b385d3c639ef483525f07</td>\n",
       "      <td>there is hope</td>\n",
       "      <td>opdivo</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4530</th>\n",
       "      <td>847039a8505f7a6f6db560a43009426204e8d835</td>\n",
       "      <td>all the best in the new year</td>\n",
       "      <td>opdivo</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5118</th>\n",
       "      <td>2e0b9e15f5ae2fb7e6b5c7eecdff6a967b566a16</td>\n",
       "      <td>good luck with your wife and god bless</td>\n",
       "      <td>opdivo</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5276</th>\n",
       "      <td>e9159c3f836f453f5f133008d40b7e65f9e70a42</td>\n",
       "      <td>all the best to your husband and family</td>\n",
       "      <td>opdivo</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   unique_hash  \\\n",
       "32    50b51148821b2d0eb23380ca8417b1d9fefd37af   \n",
       "1856  ddb40991ffb9cddac847c77855977610bf4d03ae   \n",
       "1973  ae868203b1cebf2c75722bc38c1f7611ad706ab0   \n",
       "2301  6df111aa3032069076b12f81bd87a9d72dc795ad   \n",
       "2894  f275e928df77f875dc61f97c791b40519effc58c   \n",
       "3446  f57ddcc186c8557ad4201bb252244bba749f23cf   \n",
       "3525  65fb06982b3518557c8244b947fb36ae33b15534   \n",
       "3720  c40d519b9b37c577ff6b385d3c639ef483525f07   \n",
       "4530  847039a8505f7a6f6db560a43009426204e8d835   \n",
       "5118  2e0b9e15f5ae2fb7e6b5c7eecdff6a967b566a16   \n",
       "5276  e9159c3f836f453f5f133008d40b7e65f9e70a42   \n",
       "\n",
       "                                                   text       drug  sentiment  \\\n",
       "32     had anyone been returned to a previous chemo ...  nivolumab          1   \n",
       "1856   any ideas anyone   i m not due to see the doc...     opdivo          2   \n",
       "1973                                      be well          tarceva          1   \n",
       "2301                                              sandy     opdivo          2   \n",
       "2894   i hope and pray you guys have the success i h...     opdivo          2   \n",
       "3446                                        any advice      opdivo          2   \n",
       "3525                                              chuck     opdivo          2   \n",
       "3720                                     there is hope      opdivo          2   \n",
       "4530                      all the best in the new year      opdivo          0   \n",
       "5118            good luck with your wife and god bless      opdivo          0   \n",
       "5276           all the best to your husband and family      opdivo          2   \n",
       "\n",
       "     occurences  occurences_count  \n",
       "32           []                 0  \n",
       "1856         []                 0  \n",
       "1973         []                 0  \n",
       "2301         []                 0  \n",
       "2894         []                 0  \n",
       "3446         []                 0  \n",
       "3525         []                 0  \n",
       "3720         []                 0  \n",
       "4530         []                 0  \n",
       "5118         []                 0  \n",
       "5276         []                 0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_hash</th>\n",
       "      <th>text</th>\n",
       "      <th>drug</th>\n",
       "      <th>occurences</th>\n",
       "      <th>occurences_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>d3fb8ee77b4a9797b6bd2d793429901be168ca64</td>\n",
       "      <td>these are immunolgy drugs and work against th...</td>\n",
       "      <td>opdivo</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1862</th>\n",
       "      <td>231b3cc1744ebd896fa2420ba2ab7b7e35c8ab05</td>\n",
       "      <td>you have to keep taking it even in remission ...</td>\n",
       "      <td>opdivo</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2542</th>\n",
       "      <td>a07303c7e50f033a7ec1c58bc2982949af63078d</td>\n",
       "      <td>3 wks later resumed low dose carbo  x3 and 35...</td>\n",
       "      <td>tarceva</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   unique_hash  \\\n",
       "421   d3fb8ee77b4a9797b6bd2d793429901be168ca64   \n",
       "1862  231b3cc1744ebd896fa2420ba2ab7b7e35c8ab05   \n",
       "2542  a07303c7e50f033a7ec1c58bc2982949af63078d   \n",
       "\n",
       "                                                   text     drug occurences  \\\n",
       "421    these are immunolgy drugs and work against th...   opdivo         []   \n",
       "1862   you have to keep taking it even in remission ...   opdivo         []   \n",
       "2542   3 wks later resumed low dose carbo  x3 and 35...  tarceva         []   \n",
       "\n",
       "      occurences_count  \n",
       "421                  0  \n",
       "1862                 0  \n",
       "2542                 0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.occurences_count==0]\n",
    "df_test[df_test.occurences_count==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T17:36:00.728642Z",
     "start_time": "2019-07-24T17:35:57.429830Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"context_text\"] = get_surrounding_text(df,surround)\n",
    "df_test[\"context_text\"] = get_surrounding_text(df_test,surround)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 162.  ,  534.  ,  862.85, 2126.63])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([ 157.  ,  514.2 ,  768.1 , 1994.92])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([  860.5,  4714.3,  9219. , 27821. ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([  807.  ,  4191.6 ,  8327.5 , 21694.28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.occurences_count.value_counts() [df.occurences_count<10]\n",
    "\n",
    "np.percentile(df_test['context_text'].apply(len).values,[50,90,95,99])\n",
    "\n",
    "np.percentile(df['context_text'].apply(len).values,[50,90,95,99])\n",
    "\n",
    "np.percentile(df_test['text'].apply(len).values,[50,90,95,99])\n",
    "\n",
    "np.percentile(df['text'].apply(len).values,[50,90,95,99])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T17:39:27.045016Z",
     "start_time": "2019-07-24T17:36:00.730928Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5279/5279 [01:24<00:00, 62.71it/s] \n",
      "100%|██████████| 2924/2924 [00:48<00:00, 33.53it/s] \n",
      "100%|██████████| 5279/5279 [00:14<00:00, 374.92it/s]\n",
      "100%|██████████| 2924/2924 [00:07<00:00, 370.99it/s]\n"
     ]
    }
   ],
   "source": [
    "df = preprocess_for_word_cnn(df, text_column='text', output_column=\"full_txt\", word_length_filter=word_length_filter, jobs=jobs)\n",
    "df_test = preprocess_for_word_cnn(df_test, text_column='text', output_column=\"full_txt\", word_length_filter=word_length_filter, jobs=jobs)\n",
    "\n",
    "df = preprocess_for_word_cnn(df, text_column='context_text', output_column=\"context_txt\", word_length_filter=word_length_filter, jobs=jobs)\n",
    "df_test = preprocess_for_word_cnn(df_test, text_column='context_text', output_column=\"context_txt\", word_length_filter=word_length_filter, jobs=jobs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T17:39:27.068876Z",
     "start_time": "2019-07-24T17:39:27.050919Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"review_length\"] = df.full_txt.apply(len)/1000\n",
    "df_test[\"review_length\"] = df_test.full_txt.apply(len)/1000\n",
    "\n",
    "df['review_length_by_occurences_count'] = df[\"review_length\"]/(df['occurences_count']+1)\n",
    "df_test['review_length_by_occurences_count'] = df_test[\"review_length\"]/(df_test['occurences_count']+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_length                        13.7350\n",
       "review_length_by_occurences_count     6.8675\n",
       "last_occurence                       64.9720\n",
       "first_occurence                      64.7330\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['last_occurence'] = df.occurences.apply(lambda x:x[-1] if len(x)>0 else -1)/1000\n",
    "df['first_occurence'] = df.occurences.apply(lambda x:x[0] if len(x)>0 else -1)/1000\n",
    "\n",
    "df_test['last_occurence'] = df_test.occurences.apply(lambda x:x[-1] if len(x)>0 else -1)/1000\n",
    "df_test['first_occurence'] = df_test.occurences.apply(lambda x:x[0] if len(x)>0 else -1)/1000\n",
    "\n",
    "df[[\"review_length\",\"review_length_by_occurences_count\",\"last_occurence\",\"first_occurence\"]].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T10:18:03.347848Z",
     "start_time": "2019-07-24T10:18:03.282988Z"
    }
   },
   "source": [
    "## Mask (Extra Channel in CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T17:39:27.153685Z",
     "start_time": "2019-07-24T17:39:27.148867Z"
    }
   },
   "outputs": [],
   "source": [
    "# arr = list(range(20))\n",
    "# make_mask_single(arr,[5,6,12],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T17:39:27.161248Z",
     "start_time": "2019-07-24T17:39:27.156932Z"
    }
   },
   "outputs": [],
   "source": [
    "# text = \"the quick brown fox jumps over the lazy dog woof bark\".split(\" \")\n",
    "# make_mask_single(text,\"over the lazy\",3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T17:39:48.103226Z",
     "start_time": "2019-07-24T17:39:27.188086Z"
    }
   },
   "outputs": [],
   "source": [
    "df = make_mask(df)\n",
    "df_test = make_mask(df_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T17:39:48.117010Z",
     "start_time": "2019-07-24T17:39:48.107915Z"
    }
   },
   "outputs": [],
   "source": [
    "# list(zip(df.head(1).full_txt.values[0],df.head(1).full_txt_mask.values[0]))\n",
    "# make_mask_single(df.head(1).full_txt.values[0],\"gilenya\",2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per word Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# less frequent words must be given values towards 0 (neutral)\n",
    "# also return occurence count of word\n",
    "# return a apply_fn\n",
    "def per_word_sentiment(df,text_col=\"\",label_col=\"\"):\n",
    "    pos_counter = Counter()\n",
    "    neg_counter = Counter()\n",
    "    neural_counter = Counter()\n",
    "    overall_counter = Counter()\n",
    "    doc_counts = dict(df.sentiment.value_counts())\n",
    "    total_docs = df.shape[0]\n",
    "    pos_docs,neg_docs,neutral_docs = doc_counts[0],doc_counts[1],doc_counts[2]\n",
    "    \n",
    "    for row in df[[text_col,label_col]].values:\n",
    "        t = set(row[0])\n",
    "        lb = row[1]\n",
    "        overall_counter.update(t)\n",
    "        if lb==0:\n",
    "            pos_counter.update(t)\n",
    "        elif lb==1:\n",
    "            neg_counter.update(t)\n",
    "        elif lb==2:\n",
    "            neural_counter.update(t)\n",
    "        else:\n",
    "            raise ValueError()\n",
    "            \n",
    "    w2s = {}\n",
    "    counts = dict(overall_counter)\n",
    "    total_pos_words = len(pos_counter)\n",
    "    total_neg_words = len(neg_counter)\n",
    "    total_words = len(overall_counter)\n",
    "    \n",
    "    for k,v in overall_counter.items():\n",
    "        pos_oc = pos_counter[k]\n",
    "        neg_oc = neg_counter[k]\n",
    "\n",
    "        pos_frac = 100*pos_oc/pos_docs\n",
    "        neg_frac = 100*neg_oc/neg_docs\n",
    "        overall_frac = 100*v/total_docs\n",
    "        sentiment = (pos_frac - neg_frac)/overall_frac\n",
    "        # sentiment = (pos_oc - neg_oc)/v\n",
    "        relative_trust = np.log10(v)*0.25\n",
    "        sentiment = sentiment*relative_trust\n",
    "        w2s[k] = sentiment\n",
    "#         if overall_frac>2:\n",
    "#             print(k,overall_frac)\n",
    "        if k=='doctor':\n",
    "            print(pos_oc,neg_oc,v)\n",
    "            print(pos_frac,neg_frac,overall_frac,relative_trust,sentiment)\n",
    "    def apply_w2s_fn(word_array):\n",
    "        return list(map(lambda w:w2s[w] if w in w2s else 0.0,word_array))\n",
    "    def apply_counts_fn(word_array):\n",
    "        return list(map(lambda w:counts[w]/1000 if w in w2s else 0.0,word_array))\n",
    "        \n",
    "    return w2s,counts,apply_w2s_fn,apply_counts_fn\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 140 824\n",
      "11.021069692058347 16.726403823178018 15.609016859253646 0.7289818029242789 -0.2664539860960795\n"
     ]
    }
   ],
   "source": [
    "w2s,counts,apply_w2s_fn,apply_counts_fn = per_word_sentiment(df,\"full_txt\",\"sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.36587143013623225"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "31907"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2    64\n",
       "0    10\n",
       "1     4\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[-0.2664539860960795, 0.36587143013623225]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = 'retinopathy'\n",
    "counts[w]\n",
    "w2s[w]\n",
    "len(counts)\n",
    "df[df.full_txt.apply(lambda x:w in x)]['sentiment'].value_counts()\n",
    "\n",
    "apply_w2s_fn([\"doctor\",\"retinopathy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"word_sentiments\"] = df.full_txt.apply(apply_w2s_fn)\n",
    "df_test[\"word_sentiments\"] = df_test.full_txt.apply(apply_w2s_fn)\n",
    "\n",
    "df[\"word_count\"] = df.full_txt.apply(apply_counts_fn)\n",
    "df_test[\"word_count\"] = df_test.full_txt.apply(apply_counts_fn)\n",
    "\n",
    "df[\"word_sentiments_ctx\"] = df.context_txt.apply(apply_w2s_fn)\n",
    "df_test[\"word_sentiments_ctx\"] = df_test.context_txt.apply(apply_w2s_fn)\n",
    "\n",
    "df[\"word_count_ctx\"] = df.context_txt.apply(apply_counts_fn)\n",
    "df_test[\"word_count_ctx\"] = df_test.context_txt.apply(apply_counts_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2911763583107378"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3.265"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(df[\"word_sentiments\"].apply(np.max))\n",
    "np.max(df[\"word_count\"].apply(np.max))\n",
    "\n",
    "# np.max(df[\"word_sentiments_ctx\"].apply(np.max))\n",
    "# np.max(df[\"word_count_ctx\"].apply(np.max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How many text have 2 or more drug mention?\n",
    "- How many reviews are used twice in dataset?\n",
    "- Max, Min, Median, Mean sentence length\n",
    "- Total number of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T12:23:13.507771Z",
     "start_time": "2019-07-24T12:23:13.500473Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['unique_hash', 'text', 'drug', 'sentiment', 'occurences',\n",
       "       'occurences_count', 'context_text', 'full_txt', 'context_txt',\n",
       "       'review_length', 'review_length_by_occurences_count', 'last_occurence',\n",
       "       'first_occurence', 'full_txt_mask', 'full_txt_mask_gaussian',\n",
       "       'word_sentiments', 'word_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    3825\n",
       "1     837\n",
       "0     617\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T16:24:00.342906Z",
     "start_time": "2019-07-24T16:24:00.335452Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 414.  ,  831.2 , 2192.56])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([ 461.1 ,  922.55, 2748.69])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(df['full_txt'].apply(len),[90,95,99])\n",
    "np.percentile(df_test['full_txt'].apply(len),[90,95,99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T16:24:00.376743Z",
     "start_time": "2019-07-24T16:24:00.346776Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>180.581848</td>\n",
       "      <td>66</td>\n",
       "      <td>3</td>\n",
       "      <td>4009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>135.354839</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>1844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>225.993203</td>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>13735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 mean  median  min    max\n",
       "sentiment                                \n",
       "0          180.581848      66    3   4009\n",
       "1          135.354839      70    1   1844\n",
       "2          225.993203      81    1  13735"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([\"sentiment\"])[\"review_length\"].agg(['mean',\"median\",\"min\",\"max\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T09:12:04.779601Z",
     "start_time": "2019-07-24T09:12:04.768196Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'afainib',\n",
       " 'alitma',\n",
       " 'duvalumab',\n",
       " 'elotinib',\n",
       " 'etrolizumab',\n",
       " 'flixabi',\n",
       " 'osmertinib',\n",
       " 'risankizumab',\n",
       " 'stellara'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drug overlap in train test set\n",
    "len(set(df.drug.values))\n",
    "len(set(df_test.drug.values))\n",
    "set(df_test.drug.values) - set(df.drug.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T17:39:49.289992Z",
     "start_time": "2019-07-24T17:39:48.120167Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31907"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "24571"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Exclusive Distinct words =  5618\n",
      "Total vocab size =  37525\n"
     ]
    }
   ],
   "source": [
    "# How many words in test set are not in train set\n",
    "\n",
    "train_words = set(list(more_itertools.flatten(df.full_txt.values)))\n",
    "test_words = set(list(more_itertools.flatten(df_test.full_txt.values)))\n",
    "all_words = set(list(more_itertools.flatten(df.full_txt.values))+list(more_itertools.flatten(df_test.full_txt.values)))\n",
    "len(train_words)\n",
    "len(test_words)\n",
    "test_exlusive_words = test_words-train_words\n",
    "\n",
    "print(\"Test Set Exclusive Distinct words = \",len(test_exlusive_words))\n",
    "\n",
    "print(\"Total vocab size = \",len(all_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Building Fasttext Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fname = os.getcwd()+\"/fasttext.model\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "crawl = datapath(os.getcwd()+\"/crawl-300d-2M-subword.bin\")\n",
    "fb_model = load_facebook_model(crawl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in FB Fasttext Model =  37525\n",
      "Words Not in FB Fasttext Model =  0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "success = 0\n",
    "fail = 0\n",
    "for w in all_words:\n",
    "\n",
    "    if w in fb_model.wv:\n",
    "        success = success + 1\n",
    "    else:\n",
    "        fail = fail + 1\n",
    "\n",
    "print(\"Words in FB Fasttext Model = \",success)\n",
    "print(\"Words Not in FB Fasttext Model = \", fail)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8203"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = list(df.full_txt.values) + list(df_test.full_txt.values)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fb_model.build_vocab(sentences=sentences, update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8203000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "8203"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fb_model.corpus_total_words\n",
    "fb_model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "fb_model.train(sentences=sentences, \n",
    "               total_examples=fb_model.corpus_count, epochs=1, total_words=fb_model.corpus_total_words)\n",
    "fb_model.save(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "fb_model = FastText.load(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding, Padding and Saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T17:41:15.111197Z",
     "start_time": "2019-07-24T17:41:15.040234Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['unique_hash', 'text', 'drug', 'sentiment', 'occurences',\n",
       "       'occurences_count', 'context_text', 'full_txt', 'context_txt',\n",
       "       'review_length', 'review_length_by_occurences_count', 'last_occurence',\n",
       "       'first_occurence', 'full_txt_mask', 'full_txt_mask_gaussian',\n",
       "       'word_sentiments', 'word_count', 'word_sentiments_ctx',\n",
       "       'word_count_ctx'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n",
    "\n",
    "df['ohe_labels'] = list(np_utils.to_categorical(df['sentiment'], 3))\n",
    "df['ohe_labels'] = df['ohe_labels'].apply(lambda x:list(map(int,x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()\n",
    "df_test_copy = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df_copy.copy()\n",
    "# df_test = df_test_copy.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T17:39:54.961571Z",
     "start_time": "2019-07-24T17:39:54.944409Z"
    }
   },
   "outputs": [],
   "source": [
    "def do_padding(df):\n",
    "    df['full_txt'] = list(pad_text_sequences(df['full_txt'].values, maxlen=full_txt_maxlen,jobs=jobs))\n",
    "    df['context_txt'] = list(pad_text_sequences(df['context_txt'].values, maxlen=full_txt_maxlen,jobs=jobs)) # use of full_txt_maxlen is intentional\n",
    "    # we do this since we will filter out in Keras model lambda layer\n",
    "\n",
    "    df['full_txt_mask'] = list(pad_text_sequences(df['full_txt_mask'].values,empty=0.0, maxlen=full_txt_maxlen,jobs=jobs))\n",
    "    df['full_txt_mask_gaussian'] = list(pad_text_sequences(df['full_txt_mask_gaussian'].values,empty=0.0, maxlen=full_txt_maxlen,jobs=jobs))\n",
    "    \n",
    "    \n",
    "    df['word_sentiments'] = list(pad_text_sequences(df['word_sentiments'].values,empty=0.0, maxlen=full_txt_maxlen,jobs=jobs))\n",
    "    df['word_count'] = list(pad_text_sequences(df['word_count'].values,empty=0.0, maxlen=full_txt_maxlen,jobs=jobs))\n",
    "    df['word_sentiments_ctx'] = list(pad_text_sequences(df['word_sentiments_ctx'].values,empty=0.0, maxlen=full_txt_maxlen,jobs=jobs))\n",
    "    df['word_count_ctx'] = list(pad_text_sequences(df['word_count_ctx'].values,empty=0.0, maxlen=full_txt_maxlen,jobs=jobs))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T17:41:15.037660Z",
     "start_time": "2019-07-24T17:39:54.975505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full text length =  500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8203/8203 [00:00<00:00, 20820.83it/s]\n",
      " 17%|█▋        | 908/5279 [00:00<00:00, 9077.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words before Min frequency filtering 29815\n",
      "Total Words after Min frequency filtering 11859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5279/5279 [00:00<00:00, 8816.05it/s]\n",
      "100%|██████████| 2924/2924 [00:00<00:00, 7925.57it/s]\n",
      "100%|██████████| 5279/5279 [00:00<00:00, 10390.82it/s]\n",
      "100%|██████████| 2924/2924 [00:00<00:00, 9326.42it/s] \n"
     ]
    }
   ],
   "source": [
    "print(\"full text length = \",full_txt_maxlen)\n",
    "df = do_padding(df)\n",
    "df_test = do_padding(df_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"train_padded.csv\",index=False)\n",
    "df_test.to_csv(\"test_padded.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.to_csv(\"train_non_padded.csv\",index=False)\n",
    "df_test_copy.to_csv(\"test_non_padded.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['unique_hash', 'text', 'drug', 'sentiment', 'occurences',\n",
       "       'occurences_count', 'context_text', 'full_txt', 'context_txt',\n",
       "       'review_length', 'review_length_by_occurences_count', 'last_occurence',\n",
       "       'first_occurence', 'full_txt_mask', 'full_txt_mask_gaussian',\n",
       "       'word_sentiments', 'word_count', 'word_sentiments_ctx',\n",
       "       'word_count_ctx', 'ohe_labels', 'full_txt_encoded',\n",
       "       'context_txt_encoded'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Index(['unique_hash', 'text', 'drug', 'sentiment', 'occurences',\n",
       "       'occurences_count', 'context_text', 'full_txt', 'context_txt',\n",
       "       'review_length', 'review_length_by_occurences_count', 'last_occurence',\n",
       "       'first_occurence', 'full_txt_mask', 'full_txt_mask_gaussian',\n",
       "       'word_sentiments', 'word_count', 'word_sentiments_ctx',\n",
       "       'word_count_ctx', 'ohe_labels'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n",
    "df_copy.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "320px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
